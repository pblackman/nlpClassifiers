{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.optim import Adam\n",
    "from nlpClassifiers.data.dataset  import NLPDataset, Vocabulary\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from scipy.special import expit\n",
    "import gensim\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '../../'\n",
    "PATH_TO_VIRTUAL_OPERATOR_DATA = join(ROOT, \"data/virtual-operator\")\n",
    "PATH_TO_AGENT_BENCHMARK_DATA = join(ROOT, \"data/agent-benchmark\")\n",
    "PATH_TO_ML_PT_DATA = join(ROOT, \"data/mercado-livre-pt-only\")\n",
    "\n",
    "PATH_TO_VIRTUAL_OPERATOR_MODELS = join(ROOT, \"models/virtual-operator\")\n",
    "PATH_TO_AGENT_BENCHMARK_MODELS = join(ROOT, \"models/agent-benchmark\")\n",
    "PATH_TO_ML_PT_MODELS = join(ROOT, \"models/mercado-livre-pt-only\")\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "EMBEDDINGS_FILE = '../../data/word2vec/cbow_s300_pt.txt'\n",
    "MAX_WORDS = 30000\n",
    "dataset = 'mercado-livre-pt'\n",
    "sentence_max_len = 82\n",
    "batch_size = 512\n",
    "\n",
    "epochs=30\n",
    "stopwords_lang = None\n",
    "gpu=5\n",
    "lr=0.001\n",
    "clipping_value=0.25\n",
    "save_name='cnn-pytorch-word2vec-public-pt-br'\n",
    "patience=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_TO_MODELS = {\"virtual-operator\": PATH_TO_VIRTUAL_OPERATOR_MODELS, \"agent-benchmark\": PATH_TO_AGENT_BENCHMARK_MODELS, \"mercado-livre-pt\": PATH_TO_ML_PT_MODELS}\n",
    "FULL_PATH_TO_MODELS = join(BASE_PATH_TO_MODELS[dataset], \"bow-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset, subset):\n",
    "    BASE_PATH_TO_DATASET = {\"virtual-operator\": PATH_TO_VIRTUAL_OPERATOR_DATA, \"agent-benchmark\": PATH_TO_AGENT_BENCHMARK_DATA, \"mercado-livre-pt\": PATH_TO_ML_PT_DATA}\n",
    "    BASE_PATH_TO_DATASET = {\"train\": join(BASE_PATH_TO_DATASET[dataset], \"train.csv\"), \"val\": join(BASE_PATH_TO_DATASET[dataset], \"val.csv\"), \"test\": join(BASE_PATH_TO_DATASET[dataset], \"test.csv\")}\n",
    "    FULL_PATH_TO_DATASET = BASE_PATH_TO_DATASET[subset]\n",
    "    \n",
    "    if dataset == \"mercado-livre-pt\":\n",
    "        sep=\",\"\n",
    "    else:\n",
    "        sep=\";\"\n",
    "    data = pd.read_csv(FULL_PATH_TO_DATASET, sep=sep, names =['utterance','label'], header=None, dtype={'utterance':str, 'label': str} )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = read_data(dataset, \"train\")\n",
    "val_df = read_data(dataset, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12917 tokens not found in vocabulary.\n"
     ]
    }
   ],
   "source": [
    "voc = Vocabulary('CNN', stopwords_lang)\n",
    "voc.build_vocab(train_df['utterance'].tolist() +  val_df['utterance'].tolist(), MAX_WORDS)\n",
    "embedding_weights = voc.load_embeddings(EMBEDDINGS_FILE, EMBEDDING_DIM)\n",
    "if voc.num_words < MAX_WORDS or MAX_WORDS == 0:\n",
    "    MAX_WORDS = voc.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = NLPDataset(dataset, \"train\", sentence_max_len, vocab= voc)\n",
    "labels_dict = train_corpus.labels_dict\n",
    "val_corpus = NLPDataset(dataset, \"val\", sentence_max_len, labels_dict = labels_dict, vocab= voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train_corpus,\n",
    "            sampler = RandomSampler(train_corpus),\n",
    "            batch_size = batch_size,\n",
    "            pin_memory=True,\n",
    "            worker_init_fn=_init_fn,\n",
    "            num_workers=0\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_corpus,\n",
    "            sampler = RandomSampler(val_corpus),\n",
    "            batch_size = batch_size,\n",
    "            pin_memory=True,\n",
    "            worker_init_fn=_init_fn,\n",
    "            num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    if type(model) in [nn.Linear]:\n",
    "        nn.init.xavier_normal_(model.weight.data)\n",
    "    elif type(model) in [nn.LSTM, nn.RNN, nn.GRU]:\n",
    "        nn.init.xavier_normal_(model.weight_hh_l0)\n",
    "        nn.init.xavier_normal_(model.weight_ih_l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=10):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    def __init__(self, num_classes, num_features, criterion, embedding_dim, embedding_weights=None):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = 1\n",
    "        self.embedding = nn.Embedding(num_features, embedding_dim, padding_idx = 0)\n",
    "        if(embedding_weights is not None):\n",
    "            print(\"Embedding layer Weights won't be updated.\")\n",
    "            self.embedding.load_state_dict({'weight': embedding_weights})\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            #self.embedding.from_pretrained(embedding_weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding.weight.data = torch.zeros(self.embedding.weight.data.size())\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        self.cnn = nn.Conv1d(embedding_dim, 256, 4)\n",
    "        \n",
    "        for name, param in self.cnn.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "             \n",
    "        self.dropout = nn.Dropout(0.2)       \n",
    "        self.out = nn.Linear(256, num_classes)\n",
    "        self.init_weights()\n",
    "    def forward(self, x, y):\n",
    "        # Conv1d takes in (batch, channels, seq_len), but raw embedded is (batch, seq_len, channels)\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = h_embedding.permute(0, 2, 1)\n",
    "        h_cnn = self.cnn(h_embedding)\n",
    "        h_cnn = h_cnn.permute(0, 2, 1)\n",
    "        h_cnn = self.dropout(h_cnn)\n",
    "        h_max_pool = self.global_max_pool(h_cnn)\n",
    "        x = self.out(h_max_pool)\n",
    "        loss = self.criterion(x, y)\n",
    "        # return the final output\n",
    "        return loss,x \n",
    "    \n",
    "    @staticmethod\n",
    "    def global_max_pool(x):\n",
    "        \"\"\"Convolution and global max pooling layer\"\"\"\n",
    "        return x.max(1)[0]\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Here we reproduce Keras default initialization weights to initialize Embeddings/LSTM weights\n",
    "        \"\"\"\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for t in ih:\n",
    "            nn.init.xavier_uniform(t)\n",
    "        for t in hh:\n",
    "            nn.init.orthogonal(t)\n",
    "        for t in b:\n",
    "            nn.init.constant(t, 0)\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.embedding_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.embedding_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer Weights won't be updated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = CNNNet(train_corpus.num_labels, MAX_WORDS, criterion, EMBEDDING_DIM, embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNNet(\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (embedding): Embedding(30000, 300, padding_idx=0)\n",
       "  (cnn): Conv1d(300, 256, kernel_size=(4,), stride=(1,))\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (out): Linear(in_features=256, out_features=1048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optimizer = Adam(model.parameters(),lr, betas=(0.7, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    acc = acc * 100.0\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    acc = (labels.cpu() == logits.cpu().argmax(-1)).float().detach().numpy()\n",
    "    return float(100 * acc.sum() / len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    975.    .\n",
      "  step loss: 5.60\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 4.92\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 3.98\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 3.53\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 2.93\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 2.68\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 2.64\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 2.35\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 2.20\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 1.82\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 1.87\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 1.93\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 1.57\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 1.66\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 1.47\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 1.49\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 1.35\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 1.32\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 1.22\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 1.30\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 1.25\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 1.16\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 1.19\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 1.17\n",
      "\n",
      "  Average training loss: 2.30\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 83.20\n",
      "Batch accuracy: 78.32\n",
      "Batch accuracy: 76.95\n",
      "Batch accuracy: 83.20\n",
      "Batch accuracy: 78.52\n",
      "Batch accuracy: 81.25\n",
      "Batch accuracy: 78.91\n",
      "Batch accuracy: 80.86\n",
      "Batch accuracy: 78.12\n",
      "Batch accuracy: 80.66\n",
      "Batch accuracy: 79.88\n",
      "Batch accuracy: 75.98\n",
      "Batch accuracy: 76.56\n",
      "Batch accuracy: 79.88\n",
      "Batch accuracy: 78.71\n",
      "Batch accuracy: 80.08\n",
      "Batch accuracy: 80.27\n",
      "Batch accuracy: 74.80\n",
      "Batch accuracy: 77.15\n",
      "Batch accuracy: 79.88\n",
      "Batch accuracy: 76.37\n",
      "Batch accuracy: 79.30\n",
      "Batch accuracy: 81.64\n",
      "Batch accuracy: 77.34\n",
      "Batch accuracy: 79.49\n",
      "Batch accuracy: 79.69\n",
      "Batch accuracy: 82.62\n",
      "Batch accuracy: 80.08\n",
      "Batch accuracy: 76.95\n",
      "Batch accuracy: 77.73\n",
      "Batch accuracy: 80.47\n",
      "Batch accuracy: 78.32\n",
      "Batch accuracy: 82.23\n",
      "Batch accuracy: 80.08\n",
      "Batch accuracy: 74.02\n",
      "Batch accuracy: 75.59\n",
      "Batch accuracy: 78.32\n",
      "Batch accuracy: 78.71\n",
      "Batch accuracy: 79.30\n",
      "Batch accuracy: 77.54\n",
      "Batch accuracy: 79.88\n",
      "Batch accuracy: 80.47\n",
      "Batch accuracy: 81.05\n",
      "Batch accuracy: 81.05\n",
      "Batch accuracy: 78.52\n",
      "Batch accuracy: 82.62\n",
      "Batch accuracy: 79.69\n",
      "Batch accuracy: 78.91\n",
      "Batch accuracy: 79.88\n",
      "Batch accuracy: 81.25\n",
      "Batch accuracy: 76.56\n",
      "Batch accuracy: 81.84\n",
      "Batch accuracy: 78.52\n",
      "Batch accuracy: 80.27\n",
      "Batch accuracy: 77.73\n",
      "Batch accuracy: 82.03\n",
      "Batch accuracy: 76.17\n",
      "Batch accuracy: 79.30\n",
      "Batch accuracy: 79.10\n",
      "Batch accuracy: 79.69\n",
      "Batch accuracy: 77.15\n",
      "Batch accuracy: 77.73\n",
      "Batch accuracy: 78.71\n",
      "Batch accuracy: 80.66\n",
      "Batch accuracy: 76.37\n",
      "Batch accuracy: 81.45\n",
      "Batch accuracy: 77.73\n",
      "Batch accuracy: 80.86\n",
      "Batch accuracy: 77.54\n",
      "Batch accuracy: 78.71\n",
      "Batch accuracy: 81.25\n",
      "Batch accuracy: 80.27\n",
      "Batch accuracy: 76.95\n",
      "Batch accuracy: 79.49\n",
      "Batch accuracy: 78.71\n",
      "Batch accuracy: 74.80\n",
      "Batch accuracy: 78.91\n",
      "Batch accuracy: 80.66\n",
      "Batch accuracy: 77.15\n",
      "Batch accuracy: 77.73\n",
      "Batch accuracy: 77.93\n",
      "Batch accuracy: 79.10\n",
      "Batch accuracy: 81.45\n",
      "Batch accuracy: 77.54\n",
      "Batch accuracy: 81.25\n",
      "Batch accuracy: 77.34\n",
      "Batch accuracy: 79.49\n",
      "Batch accuracy: 81.84\n",
      "Batch accuracy: 80.47\n",
      "Batch accuracy: 80.66\n",
      "Batch accuracy: 77.93\n",
      "Batch accuracy: 76.56\n",
      "Batch accuracy: 80.47\n",
      "Batch accuracy: 77.54\n",
      "Batch accuracy: 77.34\n",
      "Batch accuracy: 77.54\n",
      "Batch accuracy: 79.30\n",
      "Batch accuracy: 76.37\n",
      "Batch accuracy: 77.15\n",
      "Batch accuracy: 81.05\n",
      "Batch accuracy: 81.45\n",
      "Batch accuracy: 80.86\n",
      "Batch accuracy: 79.69\n",
      "Batch accuracy: 79.10\n",
      "Batch accuracy: 75.59\n",
      "Batch accuracy: 78.91\n",
      "Batch accuracy: 78.12\n",
      "Batch accuracy: 80.27\n",
      "Batch accuracy: 85.48\n",
      "  Accuracy: 79.12\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 2 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 1.00\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 1.06\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.99\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.96\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 1.04\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.88\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.96\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.88\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.84\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.96\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.90\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.77\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.96\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.94\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.81\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.80\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.76\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.90\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.69\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.71\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.85\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.70\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.74\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.74\n",
      "\n",
      "  Average training loss: 0.87\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 84.38\n",
      "Batch accuracy: 83.59\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 81.45\n",
      "Batch accuracy: 84.96\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 84.18\n",
      "Batch accuracy: 82.03\n",
      "Batch accuracy: 82.81\n",
      "Batch accuracy: 84.57\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 84.77\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 84.38\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 85.16\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 82.62\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 83.40\n",
      "Batch accuracy: 83.79\n",
      "Batch accuracy: 83.79\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 83.20\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 84.38\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 83.59\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 81.64\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 83.79\n",
      "Batch accuracy: 83.20\n",
      "Batch accuracy: 84.77\n",
      "Batch accuracy: 84.96\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 83.01\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 83.59\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 82.42\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 83.40\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 83.40\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 84.96\n",
      "Batch accuracy: 79.49\n",
      "Batch accuracy: 82.03\n",
      "Batch accuracy: 83.40\n",
      "Batch accuracy: 84.96\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 84.38\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 80.27\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 83.79\n",
      "Batch accuracy: 84.57\n",
      "Batch accuracy: 84.38\n",
      "Batch accuracy: 82.42\n",
      "Batch accuracy: 84.38\n",
      "Batch accuracy: 83.79\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 83.20\n",
      "Batch accuracy: 83.01\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 83.40\n",
      "Batch accuracy: 85.16\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 83.59\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 81.84\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 84.77\n",
      "Batch accuracy: 82.42\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 84.18\n",
      "Batch accuracy: 82.23\n",
      "Batch accuracy: 82.62\n",
      "Batch accuracy: 84.18\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 83.79\n",
      "Batch accuracy: 85.48\n",
      "  Accuracy: 84.89\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 3 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.66\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.59\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.71\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.74\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.57\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.71\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.79\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.72\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.58\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.53\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.63\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.63\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.72\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.62\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.56\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.70\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.60\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.58\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.59\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.73\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.61\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.70\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.60\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.62\n",
      "\n",
      "  Average training loss: 0.64\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 84.38\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 84.77\n",
      "Batch accuracy: 84.96\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 85.16\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 83.79\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 84.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 84.18\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 84.38\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 83.40\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 84.57\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 85.16\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 83.06\n",
      "  Accuracy: 87.05\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 4 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.57\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.56\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.46\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.60\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.55\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.59\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.51\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.51\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.48\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.46\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.56\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.55\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.54\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.54\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.60\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.57\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.62\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.42\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.49\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.55\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.42\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.50\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.58\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.49\n",
      "\n",
      "  Average training loss: 0.53\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 85.16\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 84.96\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 84.96\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 85.74\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 93.55\n",
      "  Accuracy: 88.04\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 5 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.56\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.43\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.49\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.46\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.39\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.40\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.40\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.40\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.46\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.46\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.52\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.43\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.51\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.39\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.47\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.52\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.53\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.42\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.45\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.42\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.46\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.43\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.47\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.55\n",
      "\n",
      "  Average training loss: 0.46\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 85.16\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 86.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 84.96\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 94.35\n",
      "  Accuracy: 88.58\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 6 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.32\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.46\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.40\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.39\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.33\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.48\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.37\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.41\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.47\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.36\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.39\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.46\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.37\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.39\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.42\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.35\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.63\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.33\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.37\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.35\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.47\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.43\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.33\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.49\n",
      "\n",
      "  Average training loss: 0.41\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 86.29\n",
      "  Accuracy: 89.09\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 7 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.30\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.34\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.39\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.35\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.32\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.48\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.42\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.44\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.53\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.42\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.34\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.42\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.26\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.37\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.37\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.40\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.40\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.41\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.35\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.38\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.34\n",
      "\n",
      "  Average training loss: 0.37\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 85.94\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 88.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.94\n",
      "  Accuracy: 89.49\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 8 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.28\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.40\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.32\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.37\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.27\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.30\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.33\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.39\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.35\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.37\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.36\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.38\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.37\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.36\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.34\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.56\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.36\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.39\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.30\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.40\n",
      "\n",
      "  Average training loss: 0.34\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.71\n",
      "  Accuracy: 89.47\n",
      "The model does not improve for 1 epochs!\n",
      "\n",
      "======== Epoch 9 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.38\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.37\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.28\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.27\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.35\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.33\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.33\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.34\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.32\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.33\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.32\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.37\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.34\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.36\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.24\n",
      "\n",
      "  Average training loss: 0.32\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 85.16\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.32\n",
      "  Accuracy: 89.66\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 10 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.26\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.33\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.32\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.35\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.33\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.34\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.27\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.38\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.32\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.34\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.33\n",
      "\n",
      "  Average training loss: 0.30\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.32\n",
      "  Accuracy: 89.69\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 11 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.28\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.32\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.30\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.27\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.30\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.34\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.26\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.28\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.27\n",
      "\n",
      "  Average training loss: 0.28\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.94\n",
      "  Accuracy: 89.83\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 12 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.30\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.30\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.26\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.27\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.27\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.28\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.27\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.28\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.28\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.20\n",
      "\n",
      "  Average training loss: 0.26\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.74\n",
      "  Accuracy: 89.90\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 13 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.30\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.33\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.32\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.30\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.26\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.25\n",
      "\n",
      "  Average training loss: 0.25\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.32\n",
      "  Accuracy: 89.84\n",
      "The model does not improve for 1 epochs!\n",
      "\n",
      "======== Epoch 14 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.26\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.28\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.32\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.26\n",
      "\n",
      "  Average training loss: 0.24\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 86.13\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 85.35\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.32\n",
      "  Accuracy: 89.92\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 15 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.29\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.27\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.31\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.27\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.26\n",
      "\n",
      "  Average training loss: 0.23\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 95.97\n",
      "  Accuracy: 89.88\n",
      "The model does not improve for 1 epochs!\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\n",
      "======== Epoch 16 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.26\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.30\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.26\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.24\n",
      "\n",
      "  Average training loss: 0.18\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 93.95\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 93.55\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.71\n",
      "  Accuracy: 90.31\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 17 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.28\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.16\n",
      "\n",
      "  Average training loss: 0.17\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 85.55\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.32\n",
      "  Accuracy: 90.36\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 18 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.25\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.17\n",
      "\n",
      "  Average training loss: 0.17\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 91.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 92.74\n",
      "  Accuracy: 90.40\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 19 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.14\n",
      "\n",
      "  Average training loss: 0.17\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 93.55\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 86.33\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 93.75\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.71\n",
      "  Accuracy: 90.41\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 20 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.21\n",
      "\n",
      "  Average training loss: 0.17\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 92.74\n",
      "  Accuracy: 90.45\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 21 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.11\n",
      "  Batch   520  of    975.    .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step loss: 0.16\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.24\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.10\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.13\n",
      "\n",
      "  Average training loss: 0.17\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 93.75\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 94.35\n",
      "  Accuracy: 90.48\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 22 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.10\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.21\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.15\n",
      "\n",
      "  Average training loss: 0.16\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 93.55\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.94\n",
      "  Accuracy: 90.43\n",
      "The model does not improve for 1 epochs!\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-05.\n",
      "\n",
      "======== Epoch 23 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.11\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.14\n",
      "\n",
      "  Average training loss: 0.16\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 93.75\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 86.72\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.71\n",
      "  Accuracy: 90.43\n",
      "The model does not improve for 2 epochs!\n",
      "\n",
      "======== Epoch 24 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.10\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.22\n",
      "\n",
      "  Average training loss: 0.16\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 94.92\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 93.75\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 92.74\n",
      "  Accuracy: 90.48\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 25 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.11\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.26\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.19\n",
      "\n",
      "  Average training loss: 0.16\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 83.06\n",
      "  Accuracy: 90.41\n",
      "The model does not improve for 1 epochs!\n",
      "\n",
      "======== Epoch 26 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.11\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.10\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.15\n",
      "\n",
      "  Average training loss: 0.16\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 88.71\n",
      "  Accuracy: 90.44\n",
      "The model does not improve for 2 epochs!\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-06.\n",
      "\n",
      "======== Epoch 27 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.09\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.22\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.15\n",
      "\n",
      "  Average training loss: 0.16\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 93.55\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 92.74\n",
      "  Accuracy: 90.46\n",
      "The model does not improve for 3 epochs!\n",
      "\n",
      "======== Epoch 28 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.11\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.20\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.12\n",
      "\n",
      "  Average training loss: 0.16\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 86.91\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 93.16\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 86.52\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 92.74\n",
      "  Accuracy: 90.47\n",
      "The model does not improve for 4 epochs!\n",
      "\n",
      "======== Epoch 29 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.13\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.17\n",
      "\n",
      "  Average training loss: 0.16\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 94.14\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 87.70\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 93.55\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 87.50\n",
      "Batch accuracy: 87.11\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 93.36\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 92.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 89.52\n",
      "  Accuracy: 90.44\n",
      "The model does not improve for 5 epochs!\n",
      "\n",
      "======== Epoch 30 / 30 ========\n",
      "Training...\n",
      "  Batch    40  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch    80  of    975.    .\n",
      "  step loss: 0.23\n",
      "  Batch   120  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   160  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   200  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   240  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   280  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   320  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   360  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   400  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   440  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   480  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   520  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   560  of    975.    .\n",
      "  step loss: 0.17\n",
      "  Batch   600  of    975.    .\n",
      "  step loss: 0.12\n",
      "  Batch   640  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   680  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   720  of    975.    .\n",
      "  step loss: 0.19\n",
      "  Batch   760  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   800  of    975.    .\n",
      "  step loss: 0.16\n",
      "  Batch   840  of    975.    .\n",
      "  step loss: 0.18\n",
      "  Batch   880  of    975.    .\n",
      "  step loss: 0.15\n",
      "  Batch   920  of    975.    .\n",
      "  step loss: 0.14\n",
      "  Batch   960  of    975.    .\n",
      "  step loss: 0.11\n",
      "\n",
      "  Average training loss: 0.16\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 92.77\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 91.99\n",
      "Batch accuracy: 88.28\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 92.97\n",
      "Batch accuracy: 88.48\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 88.09\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 92.38\n",
      "Batch accuracy: 94.14\n",
      "Batch accuracy: 89.45\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 92.58\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 89.65\n",
      "Batch accuracy: 91.02\n",
      "Batch accuracy: 90.43\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.82\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 91.41\n",
      "Batch accuracy: 91.60\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 89.84\n",
      "Batch accuracy: 90.23\n",
      "Batch accuracy: 88.87\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 89.26\n",
      "Batch accuracy: 88.67\n",
      "Batch accuracy: 87.89\n",
      "Batch accuracy: 92.19\n",
      "Batch accuracy: 90.62\n",
      "Batch accuracy: 91.21\n",
      "Batch accuracy: 89.06\n",
      "Batch accuracy: 90.04\n",
      "Batch accuracy: 91.94\n",
      "  Accuracy: 90.46\n",
      "The model does not improve for 6 epochs!\n",
      "====>Stopping training, the model did not improve for 6\n",
      "====>Best epoch: 24.\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3,verbose=True, factor=0.1)\n",
    "\n",
    "best_epoch = -1\n",
    "last_saved_model = \"\"\n",
    "training_stats = []\n",
    "global_step = 0\n",
    "best_val_acc = float(\"-inf\")\n",
    "best_model_wts = None\n",
    "best_curr_val = 0\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #val_h = model.init_hidden(len(batch[0]), device)\n",
    "        #val_h = tuple([each.data for each in val_h])\n",
    "         # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    .'.format(step, len(train_dataloader)))\n",
    "        b_input_ids = batch[0]\n",
    "        b_labels = batch[1].to(device)\n",
    "        b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        model.zero_grad()\n",
    "        loss, logits = model(b_input_ids, b_labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
    "        optimizer.step()\n",
    "        step_loss = loss.item()\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print(\"  step loss: {0:.2f}\".format(step_loss))\n",
    "        total_train_loss += step_loss\n",
    "        global_step += 1\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    # Measure how long this epoch took.\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0.0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        #val_h = model.init_hidden(len(batch[0]), device)\n",
    "        #val_h = tuple([each.data for each in val_h])\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        b_labels = batch[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            loss, logits = model(b_input_ids, b_labels)\n",
    "        total_eval_loss += loss\n",
    "        batch_acc = get_accuracy_from_logits(logits, b_labels)\n",
    "        batch_acc2 = multi_acc(logits, b_labels)\n",
    "        print(\"Batch accuracy: {0:.2f}\".format(batch_acc))\n",
    "        total_eval_accuracy += batch_acc\n",
    "    \n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    \n",
    "    if avg_val_accuracy > best_val_acc:\n",
    "        print(f\"New best model, saving it!\")\n",
    "        if avg_val_accuracy > best_curr_val:\n",
    "            best_curr_val = avg_val_accuracy\n",
    "        if last_saved_model:\n",
    "            shutil.rmtree(last_saved_model)\n",
    "        model_path = Path(\n",
    "            FULL_PATH_TO_MODELS,\n",
    "            f\"base-dataset-{dataset}-{save_name}\"\n",
    "        )\n",
    "        last_saved_model = model_path\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "        best_val_acc = avg_val_accuracy\n",
    "        torch.save(model, join(model_path, \"best-model.pth\"))\n",
    "        best_epoch = epoch_i\n",
    "        n_epochs_no_improvement = 0\n",
    "    elif avg_val_accuracy > best_curr_val:\n",
    "        best_curr_val = avg_val_accuracy\n",
    "        n_epochs_no_improvement = 0\n",
    "    else:\n",
    "        n_epochs_no_improvement += 1\n",
    "        print(f\"The model does not improve for {n_epochs_no_improvement} epochs!\")\n",
    "\n",
    "    if n_epochs_no_improvement > patience:\n",
    "        print(f\"====>Stopping training, the model did not improve for {n_epochs_no_improvement}\\n====>Best epoch: {best_epoch + 1}.\")\n",
    "        break\n",
    "    scheduler.step(total_eval_loss)\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model_path: Path,\n",
    "    dataset: str,\n",
    "    batch_size: int,\n",
    "    labels_dict,\n",
    "    device: torch.device\n",
    "):\n",
    "\n",
    "    print(f\"====Loading dataset for testing\")\n",
    "    test_corpus = NLPDataset(dataset, \"test\", sentence_max_len, labels_dict = labels_dict, vocab= voc)\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_corpus,\n",
    "        batch_size=batch_size,\n",
    "        #sampler = RandomSampler(test_corpus),\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    print(f\"====Loading model for testing\")\n",
    "    model = torch.load(join(model_path, \"best-model.pth\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pred_labels = []\n",
    "    test_labels = []\n",
    "    logits_list = []\n",
    "\n",
    "    def _list_from_tensor(tensor):\n",
    "        if tensor.numel() == 1:\n",
    "            return [tensor.item()]\n",
    "        return list(tensor.cpu().detach().numpy())\n",
    "\n",
    "    print(\"====Testing model...\")\n",
    "    for batch in test_dataloader:\n",
    "        #h = model.init_hidden(len(batch[0]), device)\n",
    "        #h = tuple([each.data for each in val_h])\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        b_labels = batch[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            #h = tuple([each.data for each in h])\n",
    "            loss, logits = model(b_input_ids, b_labels)\n",
    "            preds = np.argmax(logits.cpu(), axis=1) # Convert one-hot to index\n",
    "            b_labels = b_labels.int()\n",
    "            pred_labels.extend(_list_from_tensor(preds))\n",
    "            test_labels.extend(_list_from_tensor(b_labels))\n",
    "        logits_list.extend(_list_from_tensor(logits))\n",
    "\n",
    "    print(classification_report(test_labels, pred_labels, labels=list(labels_dict.values()), target_names=np.array(list(labels_dict.keys())), digits=3, output_dict=False))\n",
    "    logits_list = expit(logits_list)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataloader\n",
    "del validation_dataloader\n",
    "del train_corpus\n",
    "del val_corpus\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Loading dataset for testing\n",
      "====Loading model for testing\n",
      "====Testing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             precision    recall  f1-score   support\n",
      "\n",
      "                              FISHING_LINES      0.826     0.798     0.812       584\n",
      "                     MOBILE_DEVICE_CHARGERS      0.927     0.954     0.941       804\n",
      "                                 SUNGLASSES      0.946     0.955     0.951       875\n",
      "                                   FREEZERS      0.928     0.867     0.896       444\n",
      "                                 CAR_WHEELS      0.969     0.992     0.980       720\n",
      "                           BATHROOM_FAUCETS      0.860     0.860     0.860       521\n",
      "                             ACTION_FIGURES      0.648     0.629     0.638       800\n",
      "                                      IRONS      0.977     0.972     0.975       433\n",
      "                                 MATTRESSES      0.941     0.949     0.945       534\n",
      "                      SWIMMING_POOL_HEATERS      0.905     0.905     0.905        21\n",
      "                             KITCHEN_KNIVES      0.936     0.941     0.938       202\n",
      "                                TELEVISIONS      0.932     0.922     0.927       728\n",
      "                   SPORT_AND_BAZAAR_BOTTLES      0.847     0.873     0.860       229\n",
      "                                   STARTERS      0.950     0.966     0.958       354\n",
      "                                 FACE_MASKS      0.806     0.777     0.791       166\n",
      "                            KITCHEN_FAUCETS      0.878     0.886     0.882       431\n",
      "    TACTICAL_AND_SPORTING_KNIVES_AND_BLADES      0.933     0.939     0.936       164\n",
      "                                  NOTEBOOKS      0.903     0.941     0.922       543\n",
      "                                DEEP_FRYERS      0.874     0.912     0.892       455\n",
      "                               TORSION_BARS      1.000     0.992     0.996       123\n",
      "                           AUDIO_AMPLIFIERS      0.788     0.788     0.788       311\n",
      "                             CEILING_LIGHTS      0.925     0.943     0.934       472\n",
      "                                      BELTS      0.902     0.939     0.920        49\n",
      "                   AUTOMOTIVE_WEATHERSTRIPS      0.941     0.966     0.953       788\n",
      "                                 HEADBOARDS      0.991     0.978     0.985       228\n",
      "                                      YARNS      0.875     0.887     0.881       142\n",
      "                               SMARTWATCHES      0.928     0.924     0.926       724\n",
      "                            FOOTBALL_SHIRTS      0.957     0.960     0.958       921\n",
      "                     AUTOMOTIVE_CLUTCH_KITS      0.978     0.983     0.981       719\n",
      "                               WRISTWATCHES      0.932     0.939     0.936       876\n",
      "                                 WALLPAPERS      0.966     0.932     0.949       885\n",
      "        AUTOMOTIVE_SIDE_VIEW_MIRROR_GLASSES      0.971     0.985     0.978       536\n",
      "                           AUDIO_INTERFACES      0.890     0.837     0.863       300\n",
      "                           KITCHEN_PLAYSETS      0.864     0.890     0.877       100\n",
      "                                LIGHT_BULBS      0.923     0.935     0.929       829\n",
      "                             CELL_BATTERIES      0.945     0.945     0.945       670\n",
      "                       SURVEILLANCE_CAMERAS      0.906     0.875     0.890       909\n",
      "                                  THERMOSES      0.755     0.740     0.747       100\n",
      "                 HUMIDIFIERS_AND_VAPORIZERS      0.891     0.896     0.893       173\n",
      "                             KITCHEN_APRONS      0.667     0.667     0.667        27\n",
      "                        PORTABLE_GENERATORS      0.977     0.985     0.981       132\n",
      "                                 CELLPHONES      0.875     0.891     0.883       329\n",
      "                           HANDICRAFT_BOXES      0.634     0.741     0.683       112\n",
      "                        EMBROIDERY_MACHINES      0.791     0.667     0.723        51\n",
      "                                    CAMERAS      0.747     0.641     0.690        92\n",
      "            DESKTOP_COMPUTER_POWER_SUPPLIES      0.969     0.978     0.974       679\n",
      "                             BABIES_FORMULA      0.818     0.920     0.866        88\n",
      "                    MOTORCYCLE_SPEEDOMETERS      0.500     0.250     0.333         4\n",
      "                                    CARPETS      0.942     0.961     0.952       857\n",
      "                      CARDS_AND_INVITATIONS      0.881     0.787     0.831        47\n",
      "                           ELECTRIC_GUITARS      0.946     0.955     0.951       778\n",
      "                          COMPUTER_MONITORS      0.947     0.961     0.954       701\n",
      "                         MULTIGAME_MACHINES      0.866     0.753     0.806        77\n",
      "                                TABLECLOTHS      0.954     0.956     0.955       544\n",
      "                                  VIBRATORS      0.794     0.726     0.759       117\n",
      "                                     PLANTS      0.946     0.952     0.949       313\n",
      "                               AM_FM_RADIOS      0.870     0.890     0.880       383\n",
      "                     ENGINE_CONTROL_MODULES      0.972     0.990     0.981       415\n",
      "            LATEX_ENAMEL_AND_ACRYLIC_PAINTS      1.000     1.000     1.000        67\n",
      "            MUSICAL_KEYBOARD_CASES_AND_BAGS      0.980     0.976     0.978       760\n",
      "                                VIDEO_GAMES      0.971     0.983     0.977       908\n",
      "                         ARTIFICIAL_FLOWERS      0.886     0.938     0.911       497\n",
      "                         OUTER_TIE_ROD_ENDS      0.972     0.976     0.974       496\n",
      "                          EXHAUST_MANIFOLDS      0.800     1.000     0.889         8\n",
      "                    DIGITAL_VOICE_RECORDERS      0.888     0.929     0.908       774\n",
      "                              REFRIGERATORS      0.926     0.933     0.930       524\n",
      "                            DISPOSABLE_CUPS      0.785     0.777     0.781       188\n",
      "                                 EYESHADOWS      0.965     0.939     0.952       738\n",
      "                                     RANGES      0.929     0.960     0.944       450\n",
      "         AUTOMOTIVE_POWER_WINDOW_REGULATORS      0.866     0.891     0.879       138\n",
      "                                    HOOKAHS      0.857     0.857     0.857       168\n",
      "                      AUTOMOTIVE_AMPLIFIERS      0.932     0.914     0.923       662\n",
      "                          PROJECTOR_SCREENS      0.962     0.943     0.952        53\n",
      "                         AUTOMOTIVE_EMBLEMS      0.923     0.910     0.916       524\n",
      "                          PAINTBALL_MARKERS      0.956     0.887     0.920        97\n",
      "              AUTOMOTIVE_SPRING_SUSPENSIONS      0.944     0.983     0.964       121\n",
      "                                ENGINE_OILS      0.949     0.962     0.956       770\n",
      "                           BATTERY_CHARGERS      0.724     0.700     0.712        30\n",
      "                    SWEATSHIRTS_AND_HOODIES      0.924     0.932     0.928       118\n",
      "                     CONTINUOUS_INK_SYSTEMS      0.971     0.964     0.968       140\n",
      "                             BATHROOM_SINKS      0.904     0.946     0.924       427\n",
      "                               MEMORY_CARDS      0.974     0.967     0.971       669\n",
      "                                   HANDBAGS      0.934     0.939     0.937       850\n",
      "                              KITCHEN_SINKS      0.882     0.885     0.884       253\n",
      "                           CAMERA_BATTERIES      0.947     0.940     0.944       554\n",
      "                            WATER_RADIATORS      0.955     0.957     0.956       399\n",
      "                          CYCLING_COMPUTERS      0.871     0.919     0.894       161\n",
      "                               EROTIC_BOOKS      0.985     0.985     0.985        67\n",
      "                           MOTORCYCLE_CASES      0.950     0.937     0.943       222\n",
      "                                   COSTUMES      0.935     0.942     0.939       276\n",
      "                          BODYWEIGHT_SCALES      0.791     0.853     0.821       293\n",
      "                     GAMEPADS_AND_JOYSTICKS      0.951     0.949     0.950       612\n",
      "                                      PANTS      0.892     0.927     0.909       799\n",
      "                                     STOOLS      0.950     0.963     0.957       594\n",
      "                              FISHING_REELS      0.971     0.951     0.961       740\n",
      "                                    WALLETS      0.970     0.961     0.965       665\n",
      "                          MUSICAL_KEYBOARDS      0.943     0.941     0.942       511\n",
      "                                 ULTRABOOKS      0.912     0.845     0.877       232\n",
      "                            ENGINE_BEARINGS      0.935     0.966     0.950       327\n",
      "                   CAR_POWER_STEERING_PUMPS      0.936     0.989     0.962       370\n",
      "                                    TABLETS      0.979     0.954     0.966       624\n",
      "                             MAKEUP_BRUSHES      0.952     0.961     0.956       534\n",
      "                              HORSE_SADDLES      0.925     0.971     0.947       102\n",
      "                         CD_AND_DVD_PLAYERS      0.876     0.876     0.876       291\n",
      "                   VIDEO_GAME_PREPAID_CARDS      0.767     0.676     0.719        68\n",
      "                            SCREEN_PRINTERS      0.930     0.936     0.933       156\n",
      "                                 ROOF_RACKS      0.970     0.979     0.974       428\n",
      "                                 HEADPHONES      0.972     0.905     0.937       189\n",
      "                      ELECTRIC_SCREWDRIVERS      0.723     0.598     0.655       122\n",
      "                            VACUUM_CLEANERS      0.929     0.812     0.867        16\n",
      "                                FLASHLIGHTS      0.968     0.962     0.965       606\n",
      "                               HOME_SHELVES      0.786     0.796     0.791       157\n",
      "                              BRUSH_CUTTERS      0.954     0.945     0.950       110\n",
      "                        COMPUTER_PROCESSORS      0.960     0.960     0.960       794\n",
      "                                   NECKTIES      0.971     0.993     0.982       134\n",
      "                         MOTORCYCLE_JACKETS      0.920     0.940     0.930       551\n",
      "                                   WRENCHES      0.930     0.968     0.948       371\n",
      "                                WHEELCHAIRS      0.947     1.000     0.973        90\n",
      "                         RAM_MEMORY_MODULES      0.982     0.973     0.977       822\n",
      "                               PENCIL_CASES      0.924     0.929     0.926       183\n",
      "               AUTOMOTIVE_SHIFT_LEVER_KNOBS      0.999     1.000     0.999       938\n",
      "                                       MUGS      0.886     0.924     0.905       329\n",
      "                             WALKIE_TALKIES      0.870     0.926     0.897       283\n",
      "                                      DOLLS      0.885     0.905     0.895       843\n",
      "                                 TREADMILLS      0.969     0.969     0.969        65\n",
      "                                SPARK_PLUGS      0.948     0.949     0.948       726\n",
      "                              HAIR_CLIPPERS      0.847     0.870     0.858       714\n",
      "                                  SUITCASES      0.951     0.971     0.961       582\n",
      "                                BLANK_DISCS      0.828     0.893     0.859       178\n",
      "                                   SPEAKERS      0.914     0.947     0.930       525\n",
      "                              DVD_RECORDERS      0.953     0.953     0.953       450\n",
      "                    KEYBOARD_AND_MOUSE_KITS      0.942     0.949     0.946       275\n",
      "                            CAR_SEAT_COVERS      0.983     0.993     0.988       942\n",
      "         AUTOMOTIVE_SUSPENSION_CONTROL_ARMS      0.965     0.980     0.973       254\n",
      "                          BAR_CODE_SCANNERS      0.984     0.972     0.978       254\n",
      "                        AUTOMOTIVE_MOLDINGS      0.958     0.974     0.966       720\n",
      "                              BABY_SWIMWEAR      0.835     0.835     0.835       121\n",
      "                       TABLE_AND_DESK_LAMPS      0.797     0.800     0.798       260\n",
      "                          JACKETS_AND_COATS      0.907     0.882     0.894       815\n",
      "                                 CHAMPAGNES      0.800     0.774     0.787        31\n",
      "                             FOOTBALL_SHOES      0.985     0.992     0.988       734\n",
      "                          CHARMS_AND_MEDALS      0.919     0.791     0.850        86\n",
      "                                MICROPHONES      0.942     0.970     0.956       465\n",
      "                        PERMANENT_EPILATORS      0.828     0.766     0.796        94\n",
      "                             WOMEN_SWIMWEAR      0.923     0.936     0.929       500\n",
      "                 PARTY_DECORATIVE_BACKDROPS      0.953     0.962     0.957       105\n",
      "                              ROLLER_SKATES      0.985     0.985     0.985       586\n",
      "                                     COMICS      0.779     0.792     0.786       534\n",
      "                                      BOOKS      0.789     0.727     0.757       834\n",
      "                             CUPCAKE_STANDS      0.500     0.500     0.500         8\n",
      "                              UPS_BATTERIES      0.703     0.703     0.703        64\n",
      "                                CAKE_STANDS      0.633     0.689     0.660        45\n",
      "           REAR_WHEEL_HUBS_BEARING_ASSEMBLY      0.978     0.990     0.984       401\n",
      "                           DIECAST_VEHICLES      0.902     0.883     0.892       606\n",
      "                                CALCULATORS      0.987     0.970     0.978       536\n",
      "                                TV_ANTENNAS      0.975     0.987     0.981       310\n",
      "                                PAPER_CLIPS      0.978     0.863     0.917        51\n",
      "                             MALE_UNDERWEAR      0.978     0.960     0.969       422\n",
      "                                 ALL_IN_ONE      0.914     0.894     0.904       189\n",
      "                           DRINKING_GLASSES      0.777     0.822     0.799       382\n",
      "                                      DRUMS      0.831     0.925     0.875       212\n",
      "                              WELDING_MASKS      0.917     0.873     0.894        63\n",
      "                                   WHISKEYS      0.926     0.944     0.935       267\n",
      "                              VIDEO_CAMERAS      0.900     0.730     0.806        37\n",
      "                              COFFEE_MAKERS      0.984     0.947     0.965       567\n",
      "                       KNEE_BRACES_SUPPORTS      0.860     0.860     0.860       129\n",
      "                                WALL_CLOCKS      0.955     0.938     0.946       480\n",
      "                                WALL_LIGHTS      0.883     0.791     0.834       172\n",
      "                             GARDEN_BENCHES      0.750     0.643     0.692        14\n",
      "                                 SCULPTURES      0.874     0.886     0.880       437\n",
      "                                       FANS      0.962     0.963     0.962       597\n",
      "                                FOUNDATIONS      0.923     0.929     0.926       706\n",
      "                                  EYELINERS      0.761     0.700     0.729        50\n",
      "                                 DRILL_BITS      0.932     0.951     0.941       143\n",
      "                               PC_KEYBOARDS      0.720     0.692     0.706        26\n",
      "                              EROTIC_CREAMS      0.862     0.871     0.866       279\n",
      "                          DESKTOP_COMPUTERS      0.839     0.722     0.776        36\n",
      "                          DECORATIVE_VINYLS      0.908     0.951     0.929       709\n",
      "                                BOARD_GAMES      0.859     0.875     0.867       654\n",
      "                        KITCHEN_RANGE_HOODS      0.949     0.964     0.957       309\n",
      "                                    DRAWERS      0.885     0.894     0.889       198\n",
      "                            FURNITURE_KNOBS      0.953     0.959     0.956       147\n",
      "       HOME_APPLIANCE_CONTACTORS_AND_RELAYS      0.862     0.890     0.876       408\n",
      "                           SHAVING_MACHINES      0.822     0.773     0.797       449\n",
      "                               TOOTHBRUSHES      0.952     0.970     0.961       265\n",
      "                    STREAMING_MEDIA_DEVICES      0.914     0.923     0.918       415\n",
      "                     CAR_AC_HOSE_ASSEMBLIES      0.692     0.529     0.600        17\n",
      "                             SWAY_BAR_LINKS      0.820     0.820     0.820        89\n",
      "                              PREAMPLIFIERS      0.438     0.304     0.359        23\n",
      "                                 FISH_TANKS      0.762     0.772     0.767       145\n",
      "                                     DRONES      0.955     0.951     0.953       653\n",
      "                           EMERGENCY_LIGHTS      0.909     0.906     0.908       288\n",
      "                                   BLENDERS      0.907     0.909     0.908       452\n",
      "                             LASER_MEASURES      0.961     0.891     0.925        55\n",
      "                               CASH_DRAWERS      0.971     0.895     0.932        38\n",
      "                            CAMERA_CHARGERS      0.887     0.940     0.913       285\n",
      "                           CAR_AV_RECEIVERS      0.900     0.888     0.894       589\n",
      "                           STEERING_COLUMNS      0.940     0.986     0.962       143\n",
      "                                     SKIRTS      0.926     0.981     0.953       154\n",
      "                            SANDER_MACHINES      0.925     0.928     0.926       292\n",
      "                               RICE_COOKERS      0.927     0.981     0.953        52\n",
      "                             BRAKE_BOOSTERS      0.930     0.950     0.940       139\n",
      "                                   NETBOOKS      0.901     0.883     0.892       145\n",
      "                              FISHING_LURES      0.822     0.896     0.857        67\n",
      "                           AUTOMOTIVE_TIRES      0.808     0.738     0.771        80\n",
      "                                    STATUES      0.333     0.304     0.318        23\n",
      "                                   T_SHIRTS      0.935     0.945     0.940       742\n",
      "                            OFFICE_SOFTWARE      0.588     0.573     0.580        82\n",
      "                             BABY_CAR_SEATS      0.965     0.949     0.957       550\n",
      "                                WATCH_BANDS      0.855     0.891     0.872       265\n",
      "                                       PENS      0.700     0.778     0.737        27\n",
      "                  BATHROOM_ACCESSORIES_SETS      0.724     0.808     0.764        78\n",
      "                                CAR_STEREOS      0.803     0.807     0.805       243\n",
      " CELLPHONE_TABLET_AND_GPS_SCREEN_PROTECTORS      0.897     0.845     0.870       155\n",
      "                             GLASSES_FRAMES      0.924     0.902     0.913       296\n",
      "                               AIRSOFT_GUNS      0.967     0.951     0.959       245\n",
      "                                  HAND_FANS      0.844     0.844     0.844        32\n",
      "                                 SIDEBOARDS      0.955     0.955     0.955       201\n",
      "                           VEHICLE_STICKERS      0.953     0.964     0.959       530\n",
      "                             BABY_STROLLERS      0.942     0.972     0.957       468\n",
      "                                     MOVIES      0.794     0.704     0.746        71\n",
      "                         MICRO_ROTARY_TOOLS      0.813     0.884     0.847        69\n",
      "                                  SOUVENIRS      0.911     0.881     0.896       151\n",
      "                                     VODKAS      0.903     0.894     0.898        94\n",
      "                              PUREBRED_DOGS      0.902     0.953     0.927       705\n",
      "                                    VIOLINS      0.968     0.993     0.980       151\n",
      "                                HOVERBOARDS      0.983     0.965     0.974       480\n",
      "                     SUSPENSION_BALL_JOINTS      0.956     0.965     0.960       579\n",
      "                                  BACKPACKS      0.940     0.969     0.954       699\n",
      "                             ADHESIVE_TAPES      0.926     0.937     0.931       253\n",
      "                          ELECTRICAL_CABLES      0.964     0.976     0.970       250\n",
      "                              COFFEE_TABLES      0.952     0.769     0.851        26\n",
      "                 INTERACTIVE_GAMING_FIGURES      0.948     0.972     0.960       246\n",
      "                  AUTOMOTIVE_AC_COMPRESSORS      0.930     0.930     0.930        43\n",
      "                      HAIRDRESSING_SCISSORS      0.969     0.975     0.972       160\n",
      "                                   CUSHIONS      0.919     0.906     0.913       213\n",
      "                  THERMAL_CUPS_AND_TUMBLERS      0.804     0.661     0.725        56\n",
      "        TV_REPLACEMENT_BACKLIGHT_LED_STRIPS      0.964     0.922     0.942       115\n",
      "                        ENGINE_INTAKE_HOSES      0.898     0.941     0.919       373\n",
      "                   AUTOMOBILE_FENDER_LINERS      0.938     0.833     0.882        18\n",
      "                             DJ_CONTROLLERS      0.850     0.777     0.812       175\n",
      "                               STUFFED_TOYS      0.773     0.779     0.776       679\n",
      "             MARTIAL_ARTS_AND_BOXING_GLOVES      0.935     0.969     0.952       254\n",
      "                              CAMERA_LENSES      0.926     0.913     0.920       219\n",
      "                               EROTIC_PUMPS      0.920     0.910     0.915        89\n",
      "                              PAINT_ROLLERS      1.000     0.909     0.952        11\n",
      "                               MEN_SWIMWEAR      0.786     0.611     0.688        18\n",
      "                PORTABLE_CELLPHONE_CHARGERS      0.949     0.926     0.937       500\n",
      "                             ANALOG_CAMERAS      0.838     0.884     0.860       457\n",
      "                            HAIR_TREATMENTS      0.690     0.674     0.682        89\n",
      "                      DJ_EFFECTS_PROCESSORS      0.644     0.599     0.621       202\n",
      "                    BUMPER_IMPACT_ABSORBERS      0.974     0.935     0.954       245\n",
      "                 NOTEBOOKS_AND_WRITING_PADS      0.895     0.889     0.892       153\n",
      "                        FINGERPRINT_READERS      1.000     0.875     0.933        24\n",
      "                                   BLANKETS      0.843     0.817     0.829       131\n",
      "                                SUPPLEMENTS      0.736     0.707     0.721       430\n",
      "                                   HAMMOCKS      0.988     0.976     0.982        82\n",
      "               AUTOMOTIVE_SIDE_VIEW_MIRRORS      0.961     0.976     0.968       737\n",
      "                  FACIAL_SKIN_CARE_PRODUCTS      0.678     0.809     0.738       341\n",
      "                            PACKAGING_ROLLS      1.000     1.000     1.000        26\n",
      "                      TV_AND_MONITOR_MOUNTS      0.961     0.984     0.972       546\n",
      "                               CAR_ANTENNAS      0.999     0.996     0.997       934\n",
      "                                   CACHACAS      0.948     0.924     0.935       157\n",
      "                                    TV_SMPS      0.856     0.849     0.853       279\n",
      "                             KITCHEN_TOWELS      0.975     0.978     0.977       277\n",
      "                                   CALIPERS      0.913     0.955     0.933        22\n",
      "                            PARKING_SENSORS      0.976     0.976     0.976       539\n",
      "                  ELECTRIC_PRESSURE_WASHERS      0.968     0.968     0.968       343\n",
      "             HAIR_SHAMPOOS_AND_CONDITIONERS      0.794     0.614     0.692        44\n",
      "                               STETHOSCOPES      0.917     0.917     0.917        36\n",
      "                                   PRINTERS      0.910     0.882     0.896       400\n",
      "                            ELECTRIC_DRILLS      0.943     0.957     0.950       414\n",
      "                               KITCHEN_POTS      0.891     0.907     0.899       485\n",
      "                             FUEL_INJECTORS      0.820     0.883     0.850       103\n",
      "                           VEHICLE_SPEAKERS      0.912     0.938     0.925       421\n",
      "                          HOME_OFFICE_DESKS      0.853     0.933     0.891       149\n",
      "                             GUITAR_STRINGS      0.842     0.906     0.873       106\n",
      "                            DISC_PACKAGINGS      0.827     0.838     0.832        74\n",
      "                                LIP_GLOSSES      0.926     0.916     0.921       190\n",
      "                                    DRESSES      0.959     0.955     0.957       692\n",
      "                               BASS_GUITARS      0.935     0.929     0.932       467\n",
      "                                 TURNTABLES      0.844     0.822     0.833       270\n",
      "                          TOY_BUILDING_SETS      0.829     0.835     0.832       249\n",
      "                                 BED_SHEETS      0.744     0.813     0.777        75\n",
      "                              CAR_GEARBOXES      0.960     0.985     0.972       267\n",
      "                               FLOOD_LIGHTS      0.947     0.959     0.953       462\n",
      "                         SOLDERING_MACHINES      0.961     0.961     0.961       383\n",
      "                           ELECTRONIC_DRUMS      0.842     0.821     0.831       117\n",
      "                   DATA_CABLES_AND_ADAPTERS      0.847     0.900     0.873        80\n",
      "                     AUTOMOTIVE_WATER_PUMPS      0.920     0.885     0.902       391\n",
      "                             FOOTBALL_BALLS      0.949     0.962     0.955       156\n",
      "                               LENS_FILTERS      0.886     0.933     0.909        75\n",
      "                      INSTRUMENT_AMPLIFIERS      0.884     0.882     0.883       449\n",
      "                               MUSIC_STANDS      0.964     0.844     0.900        32\n",
      "                       CAR_DISTRIBUTOR_CAPS      0.949     0.912     0.930       205\n",
      "                                 MOUSE_PADS      0.886     0.891     0.888       201\n",
      "                                    FABRICS      0.879     0.897     0.888       379\n",
      "                              CABIN_FILTERS      0.984     0.996     0.990       244\n",
      "                            SEWING_MACHINES      0.886     0.908     0.897       292\n",
      "                                     MIXERS      0.971     0.951     0.961       350\n",
      "                        MOTORCYCLE_FAIRINGS      0.912     0.878     0.895       353\n",
      "                                  CV_JOINTS      0.964     0.955     0.959       443\n",
      "                 HAIR_STRAIGHTENING_BRUSHES      0.926     0.962     0.943        26\n",
      "                            WHEELS_BEARINGS      0.991     0.984     0.987       547\n",
      "                               AV_RECEIVERS      0.892     0.787     0.836       188\n",
      "                             NETWORK_CABLES      0.941     0.916     0.928       190\n",
      "                                      CRIBS      0.329     0.810     0.468       242\n",
      "                               HOME_HEATERS      0.924     0.889     0.906       234\n",
      "                 BRACELETS_AND_ANKLE_BRACES      0.873     0.853     0.863       346\n",
      "                                   SNEAKERS      0.845     0.833     0.839        72\n",
      "                 AUTOMOTIVE_THROTTLE_BODIES      0.957     0.880     0.917        50\n",
      "                             COOKING_SCALES      0.941     0.891     0.915       412\n",
      "                              PEDAL_EFFECTS      0.912     0.914     0.913       695\n",
      "                             ENGINE_PISTONS      0.769     0.780     0.774       132\n",
      "                              FLATWARE_SETS      0.867     0.839     0.852        31\n",
      "                                    PANTIES      0.840     0.857     0.848        49\n",
      "                         ELECTRICAL_OUTLETS      0.925     0.910     0.917       122\n",
      "                         BABY_CLOTHING_SETS      0.667     0.737     0.700        76\n",
      "                             STEAM_CLEANERS      0.846     0.579     0.688        19\n",
      "                           DECORATIVE_VASES      0.815     0.887     0.850       274\n",
      "                            FOOD_PROCESSORS      0.818     0.830     0.824       135\n",
      "                            TRAILER_HITCHES      0.984     0.989     0.986       184\n",
      "                      SOFA_AND_FUTON_COVERS      0.990     0.970     0.980        99\n",
      "                                     SHIRTS      0.895     0.864     0.879       118\n",
      "                      VEHICLE_CLUTCH_CABLES      1.000     1.000     1.000        10\n",
      "                                GATE_MOTORS      0.944     0.956     0.950       298\n",
      "                                 FOG_LIGHTS      0.956     0.962     0.959       292\n",
      "                                  SIM_CARDS      0.864     0.792     0.826        24\n",
      "                              BABY_MONITORS      0.969     0.986     0.977       220\n",
      "                           VEHICLE_CV_AXLES      0.942     0.942     0.942        69\n",
      "                                  LIPSTICKS      0.957     0.959     0.958       562\n",
      "                     SERVING_AND_HOME_TRAYS      0.911     0.921     0.916       354\n",
      "                                   COOKTOPS      0.750     0.545     0.632        11\n",
      "                             GRAPHICS_CARDS      0.939     0.969     0.954        32\n",
      "                         FIRE_EXTINGUISHERS      0.967     0.967     0.967        30\n",
      "                 KITCHEN_CABINET_ORGANIZERS      0.000     0.000     0.000         7\n",
      "                                      SOFAS      0.838     0.880     0.859       100\n",
      "                           GRAPHICS_TABLETS      0.931     0.926     0.929       190\n",
      "                                     SHORTS      0.908     0.942     0.925       274\n",
      "                           SWIMMING_GOGGLES      0.864     0.740     0.797        77\n",
      "                           POOL_INFLATABLES      0.850     0.919     0.883        37\n",
      "                                      DOORS      0.783     0.856     0.818        97\n",
      "                              SPORT_WATCHES      0.881     0.895     0.888       478\n",
      "           PORTABLE_EVAPORATIVE_AIR_COOLERS      0.790     0.819     0.804       138\n",
      "             DOG_CARRIERS_AND_CARRYING_BAGS      0.962     0.877     0.917        57\n",
      "                    ENGINE_INTAKE_MANIFOLDS      0.862     0.980     0.917        51\n",
      "                         MOTORCYCLE_HELMETS      0.982     0.994     0.988       715\n",
      "                       COMBUSTION_CHAINSAWS      0.975     0.929     0.951        84\n",
      "              ELECTRONIC_ENTRANCE_INTERCOMS      0.921     0.943     0.932       211\n",
      "                                    CRAYONS      0.990     0.970     0.980       101\n",
      "                               THERMOMETERS      0.800     0.889     0.842        63\n",
      "                             CAMERA_TRIPODS      0.941     0.934     0.938       427\n",
      "                          ARTIFICIAL_PLANTS      0.679     0.704     0.691        27\n",
      "                           PROJECTOR_MOUNTS      0.750     0.600     0.667         5\n",
      "                PORTABLE_ELECTRIC_MASSAGERS      0.830     0.880     0.854        83\n",
      "              NON_CORRECTIVE_CONTACT_LENSES      0.880     0.917     0.898        24\n",
      "                                    PUZZLES      0.915     0.878     0.897        74\n",
      "           WASHING_AND_DRYER_MACHINE_COVERS      0.960     0.960     0.960        50\n",
      "                       PUSH_AND_RIDING_TOYS      0.609     0.438     0.509        32\n",
      "                               SHOWER_HEADS      0.879     0.838     0.858       235\n",
      "                           ANTI_THEFT_STUDS      0.945     0.932     0.938       220\n",
      "                              HATS_AND_CAPS      0.794     0.831     0.812        65\n",
      "                                  MAGAZINES      0.679     0.622     0.650       143\n",
      "                 ENGINE_COOLING_FAN_SHROUDS      0.910     0.976     0.942        83\n",
      "                     AUTOMOTIVE_OIL_FILTERS      0.821     0.744     0.780        43\n",
      "                           MOTORCYCLE_TIRES      0.857     0.898     0.877       147\n",
      "              MOTORCYCLE_TURN_SIGNAL_LIGHTS      0.952     0.982     0.967       221\n",
      "                                    PILLOWS      1.000     0.935     0.967        31\n",
      "                                   TEQUILAS      0.941     0.970     0.955        33\n",
      "                                TOILET_RUGS      0.865     0.876     0.870       249\n",
      "                                     FLUTES      0.957     0.859     0.905       128\n",
      "                                   UKULELES      0.911     0.981     0.944        52\n",
      "                                TOOTHPASTES      0.839     0.839     0.839        62\n",
      "                      MOTORCYCLE_CRASH_BARS      0.810     0.810     0.810        21\n",
      "                            PIPES_AND_TUBES      0.444     0.308     0.364        13\n",
      "                               FOOD_SLICERS      0.900     0.818     0.857        22\n",
      "          DESKTOP_COMPUTER_COOLERS_AND_FANS      0.912     0.904     0.908       345\n",
      "                                  SWAY_BARS      0.912     0.949     0.930        98\n",
      "                           AQUARIUM_FILTERS      0.883     0.916     0.899       214\n",
      "              CELLPHONE_REPLACEMENT_CAMERAS      0.961     0.961     0.961        51\n",
      "                                    MIRRORS      0.859     0.849     0.854        86\n",
      "                            BLU_RAY_PLAYERS      0.924     0.903     0.913       175\n",
      "                         AUTOMOTIVE_FENDERS      0.897     0.774     0.831       124\n",
      "                            SPARKLING_WINES      0.714     0.417     0.526        12\n",
      "             ENGINE_VALVES_SPRING_RETAINERS      0.935     0.853     0.892        68\n",
      "                               BABY_WALKERS      0.917     0.846     0.880        13\n",
      "            ANTIVIRUS_AND_INTERNET_SECURITY      0.737     0.875     0.800        16\n",
      "                      COTTON_CANDY_MACHINES      1.000     1.000     1.000         7\n",
      "                            ORAL_IRRIGATORS      0.875     0.875     0.875         8\n",
      "                   MOTORCYCLE_CLUTCH_COVERS      0.878     0.937     0.907       239\n",
      "                              GAME_CONSOLES      0.916     0.850     0.882       347\n",
      "                              TABLE_RUNNERS      0.942     0.970     0.956        67\n",
      "                          PLAYGROUND_SLIDES      1.000     0.923     0.960        13\n",
      "                                3D_PRINTERS      0.956     0.915     0.935        47\n",
      "                                DIAPER_BAGS      0.939     0.893     0.915       103\n",
      "                              ELECTRIC_SAWS      0.962     0.975     0.969       363\n",
      "                          KITCHEN_FURNITURE      0.889     0.842     0.865        38\n",
      "                           BABY_HIGH_CHAIRS      0.642     0.489     0.555        88\n",
      "                                   MINI_PCS      0.615     0.571     0.593        14\n",
      "                                   MASCARAS      0.747     0.804     0.775        92\n",
      "                    AUTOMOTIVE_WHEEL_COVERS      0.938     0.946     0.942       129\n",
      "                          WINDSHIELD_WIPERS      0.968     0.968     0.968       190\n",
      "                                    LIQUORS      0.979     0.930     0.954       100\n",
      "                           CLOTHING_PATCHES      0.868     0.793     0.829        58\n",
      "                      WOOD_BURNING_MACHINES      1.000     1.000     1.000         7\n",
      "                                 BINOCULARS      0.534     0.683     0.599       186\n",
      "                     COMMERCIAL_LIGHT_SIGNS      0.914     1.000     0.955        32\n",
      "                           WATER_DISPENSERS      0.954     0.916     0.934       225\n",
      "                      GLOW_PLUG_CONTROLLERS      0.776     0.804     0.789        56\n",
      "                                 TELEPHONES      0.857     0.857     0.857        21\n",
      "                            AIR_COMPRESSORS      0.948     0.932     0.940       353\n",
      "                          OPERATING_SYSTEMS      0.880     0.867     0.873       270\n",
      "                       CAR_ENGINE_CAMSHAFTS      0.750     1.000     0.857        15\n",
      "                            CLOTHES_HANGERS      0.966     0.935     0.950        92\n",
      "                                   WETSUITS      0.933     0.737     0.824        19\n",
      "                               TOILET_SEATS      0.979     0.984     0.982       191\n",
      "                           AUTOMOTIVE_DOORS      0.970     0.976     0.973       332\n",
      "            SUSPENSION_CONTROL_ARM_BUSHINGS      0.813     0.790     0.801       138\n",
      "                         VEHICLE_BRAKE_PADS      0.949     0.963     0.956       350\n",
      "                REMOTE_CONTROL_TOY_VEHICLES      0.858     0.900     0.879       209\n",
      "                       AUTOMOTIVE_NERF_BARS      1.000     1.000     1.000        38\n",
      "                             ESSENTIAL_OILS      0.797     0.785     0.791        65\n",
      "                            SLATWALL_PANELS      1.000     0.975     0.987        80\n",
      "                                 TOOL_BOXES      0.910     0.927     0.918       109\n",
      "                               BABY_DIAPERS      0.989     0.973     0.981       375\n",
      "                         ALARMS_AND_SENSORS      0.895     0.895     0.895       295\n",
      "                           MOTORCYCLE_SUITS      0.861     0.838     0.849        37\n",
      "                              KITCHEN_BOWLS      0.571     0.545     0.558        22\n",
      "                             CAT_SCRATCHERS      0.818     0.600     0.692        15\n",
      "                             INK_CARTRIDGES      0.731     0.633     0.679        30\n",
      "                   MARKERS_AND_HIGHLIGHTERS      0.848     0.841     0.845       126\n",
      "                     JUMPSUITS_AND_OVERALLS      0.876     0.802     0.837       106\n",
      "                                NAIL_DRYERS      0.984     0.924     0.953       131\n",
      "                            CAMERA_MONOPODS      0.850     0.694     0.764        49\n",
      "                                   BICYCLES      0.967     0.962     0.965       184\n",
      "                                       SALT      1.000     1.000     1.000        10\n",
      "                   SCREEN_PRINTING_MACHINES      0.727     0.571     0.640        14\n",
      "                                       BEDS      0.822     0.860     0.841        43\n",
      "                    BODY_SKIN_CARE_PRODUCTS      0.709     0.750     0.729       380\n",
      "AUTOMOTIVE_MANUAL_TRANSMISSION_SHIFT_LEVERS      0.760     0.760     0.760        25\n",
      "                           TV_STORAGE_UNITS      0.865     0.865     0.865        52\n",
      "                      AUTOMOTIVE_HEADLIGHTS      0.854     0.939     0.895       131\n",
      "                                 PAINTBALLS      0.897     0.918     0.907        85\n",
      "                                     SCREWS      0.944     0.895     0.919        57\n",
      "                                 TOY_ROBOTS      0.667     0.533     0.593        15\n",
      "                                    POSTERS      0.910     0.927     0.918       206\n",
      "                           MOTORCYCLE_PANTS      0.886     0.856     0.871       118\n",
      "                        INTEGRATED_CIRCUITS      0.625     0.625     0.625        72\n",
      "                               GARDEN_HOSES      0.980     0.875     0.925        56\n",
      "                                ALTERNATORS      0.977     0.875     0.923        48\n",
      "                         MAGNIFYING_GLASSES      0.966     0.918     0.941       122\n",
      "                                   PERFUMES      0.891     0.942     0.916        52\n",
      "                                      FLOUR      1.000     0.933     0.966        15\n",
      "                               FISHING_RODS      0.704     1.000     0.826        19\n",
      "                 CELLPHONE_AND_TABLET_CASES      0.775     0.945     0.851        91\n",
      "                                    PAJAMAS      0.947     0.964     0.956        56\n",
      "                                 BUTT_PLUGS      0.659     0.794     0.720        34\n",
      "                LUMBAR_AND_ABDOMINAL_BRACES      0.947     0.966     0.956       147\n",
      "                             AIR_MATTRESSES      0.921     0.903     0.911       154\n",
      "                              WATER_HEATERS      0.971     0.943     0.957       247\n",
      "                                DISHWASHERS      0.901     0.926     0.913       148\n",
      "                       ELECTRIC_LAWN_MOWERS      1.000     0.750     0.857         4\n",
      "                              HOME_THEATERS      0.714     0.556     0.625         9\n",
      "                           LED_STAGE_LIGHTS      0.817     0.824     0.820       216\n",
      "          CHALKBOARD_AND_WHITEBOARD_ERASERS      0.952     0.870     0.909        23\n",
      "                              BABY_BOUNCERS      0.706     0.667     0.686        18\n",
      "                                      RINGS      0.907     0.907     0.907        43\n",
      "                     AUTOMOTIVE_AIR_FILTERS      0.945     0.945     0.945        55\n",
      "                               LUGGAGE_TAGS      0.891     0.872     0.882        47\n",
      "                               HEATER_CORES      1.000     0.714     0.833        21\n",
      "                            CLEANING_CLOTHS      0.866     0.903     0.884        93\n",
      "                             AIR_FRESHENERS      0.677     0.488     0.568        43\n",
      "                                    BLOUSES      0.636     0.737     0.683        38\n",
      "                            SAFETY_FOOTWEAR      0.926     0.955     0.940        66\n",
      "                               FREEZER_BAGS      0.875     0.875     0.875         8\n",
      "                     SHOCK_MOUNT_INSOLATORS      0.773     0.795     0.784        73\n",
      "                                 BAR_CLAMPS      0.846     0.688     0.759        16\n",
      "                            CHRISTMAS_TREES      0.946     0.946     0.946        37\n",
      "                         CAR_AIR_FRESHENERS      0.781     0.758     0.769        99\n",
      "                         BASKETBALL_JERSEYS      1.000     0.733     0.846        15\n",
      "                                 ACCORDIONS      0.971     0.958     0.964       311\n",
      "                                 FOOD_CARTS      1.000     0.935     0.967        31\n",
      "                                  HEAT_GUNS      0.978     0.947     0.962        94\n",
      "                  ENGINE_CRANKSHAFT_PULLEYS      0.917     0.925     0.921       107\n",
      "                                 PARTY_HATS      0.800     0.500     0.615         8\n",
      "                            RECEPTION_DESKS      1.000     1.000     1.000         4\n",
      "                             PICTURE_FRAMES      0.930     0.874     0.901       151\n",
      "                              OSCILLOSCOPES      0.889     0.800     0.842        10\n",
      "                  HABERDASHERY_LACE_EDGINGS      0.860     0.899     0.879       109\n",
      "                                      BEERS      0.958     0.947     0.953       170\n",
      "                               HOLE_PUNCHES      0.667     0.571     0.615         7\n",
      "                              DJ_TURNTABLES      0.622     0.575     0.597        80\n",
      "                                VR_HEADSETS      0.966     0.977     0.972       352\n",
      "                            KITCHEN_MORTARS      1.000     0.500     0.667         4\n",
      "                                      OVENS      0.954     0.977     0.965       171\n",
      "                               DISHES_RACKS      0.545     0.490     0.516        49\n",
      "                                   EARRINGS      0.833     0.909     0.870        44\n",
      "                                  WARDROBES      0.886     0.861     0.873        36\n",
      "                          RACKS_AND_PINIONS      0.872     0.903     0.887       113\n",
      "                                  BOOKCASES      0.829     0.829     0.829        35\n",
      "                                 TOY_TRAINS      0.838     0.713     0.770        94\n",
      "                     CARD_PAYMENT_TERMINALS      0.875     0.824     0.848        17\n",
      "                          SUBMERSIBLE_PUMPS      1.000     0.950     0.974       100\n",
      "            GARDENING_AND_AGRICULTURE_SEEDS      0.886     0.861     0.873        36\n",
      "                                     MODEMS      0.786     0.805     0.795       123\n",
      "                              OFFICE_CHAIRS      0.863     0.898     0.880        98\n",
      "                                  PENDRIVES      0.914     0.914     0.914       139\n",
      "                                  NECKLACES      0.790     0.703     0.744        91\n",
      "                         ENGINE_GASKET_SETS      0.833     0.294     0.435        17\n",
      "                 AUTOMOTIVE_SHOCK_ABSORBERS      0.784     0.879     0.829        66\n",
      "                                VINYL_ROLLS      0.657     0.629     0.642        70\n",
      "              TROLLEY_AND_FURNITURE_CASTERS      0.000     0.000     0.000         4\n",
      "                  TV_RECEIVERS_AND_DECODERS      0.714     0.833     0.769        18\n",
      "                               LIFE_JACKETS      0.882     0.682     0.769        22\n",
      "                       SELF_ADHESIVE_LABELS      0.766     0.735     0.750        49\n",
      "                  IGNITION_SWITCH_ACTUATORS      1.000     1.000     1.000        14\n",
      "                           BABY_STERILIZERS      0.737     0.636     0.683        22\n",
      "                      PARKING_BRAKE_HANDLES      0.952     0.984     0.968        61\n",
      "                     CLUTCH_SLAVE_CYLINDERS      0.759     0.733     0.746        30\n",
      "                                     FISHES      0.545     0.667     0.600         9\n",
      "                            COOKIES_CUTTERS      0.864     0.909     0.886        77\n",
      "                                 NEBULIZERS      0.778     0.857     0.816        49\n",
      "                                 COMFORTERS      0.688     0.550     0.611        20\n",
      "                        ELLIPTICAL_MACHINES      0.832     0.852     0.842       128\n",
      "                              ENGINE_VALVES      0.846     0.750     0.795        44\n",
      "                                 SANDPAPERS      0.857     0.857     0.857        21\n",
      "       AUTOMOTIVE_SHOCK_ABSORBER_BUMP_STOPS      0.969     0.838     0.899        37\n",
      "                                    GAZEBOS      0.964     0.879     0.920        91\n",
      "                                 XENON_KITS      1.000     0.500     0.667        20\n",
      "                     AUTOMOTIVE_DOOR_PANELS      0.955     0.993     0.974       150\n",
      "                 INDOOR_CURTAINS_AND_BLINDS      0.930     0.907     0.918       161\n",
      "                        CAR_WINDOW_SWITCHES      0.667     0.581     0.621        31\n",
      "                                  EPILATORS      0.777     0.777     0.777        94\n",
      "                          MALE_MASTURBATORS      0.750     0.600     0.667        15\n",
      "                        DRILLS_SCREWDRIVERS      0.450     0.375     0.409        24\n",
      "                                 CONCEALERS      0.947     0.818     0.878        88\n",
      "                    SCHOOL_AND_OFFICE_GLUES      0.667     0.600     0.632        10\n",
      "                     SANDALS_AND_FLIP_FLOPS      0.843     0.956     0.896        45\n",
      "                                  BAR_SOAPS      0.746     0.783     0.764        60\n",
      "                  ELECTRIC_BATHROOM_FAUCETS      0.714     0.800     0.755        25\n",
      "                          VOLTAGE_DETECTORS      0.933     0.966     0.949        29\n",
      "                    ORTHOPEDIC_ANKLE_BRACES      0.923     0.800     0.857        15\n",
      "                             HEDGE_TRIMMERS      1.000     1.000     1.000        14\n",
      "                        STABILIZERS_AND_UPS      1.000     0.500     0.667         2\n",
      "                              BABY_PLAYARDS      0.577     0.818     0.677       110\n",
      "                           RESISTANCE_BANDS      0.866     0.900     0.883       180\n",
      "                            LAPTOP_CHARGERS      0.877     0.838     0.857        68\n",
      "                      COMPUTER_MOTHERBOARDS      0.800     0.780     0.790        41\n",
      "                                COIN_PURSES      0.857     0.692     0.766        26\n",
      "                                      VESTS      0.857     0.928     0.891        97\n",
      "                          HAIRDRESSING_CAPS      0.750     0.750     0.750         8\n",
      "                               CAMERA_CASES      0.400     0.286     0.333         7\n",
      "                          MOTORCYCLE_GLOVES      0.849     0.934     0.890       151\n",
      "                COMPUTER_AND_TV_FLEX_CABLES      0.898     0.950     0.923       120\n",
      "                                      MANGA      0.781     0.641     0.704        39\n",
      "                               SELF_TANNERS      1.000     0.333     0.500         3\n",
      "                         HEARING_PROTECTORS      0.925     0.949     0.937        39\n",
      "                              DIFFERENTIALS      0.935     0.829     0.879        35\n",
      "                   FLEA_AND_TICK_TREATMENTS      0.900     0.907     0.903       129\n",
      "                      AUTOMOTIVE_TRUNK_LIDS      0.917     0.932     0.924       118\n",
      "                               GUITAR_PICKS      0.778     0.824     0.800        17\n",
      "                        CATS_AND_DOGS_FOODS      0.901     0.957     0.928       161\n",
      "                              BABY_BLANKETS      0.348     0.276     0.308        29\n",
      "                           CELLPHONE_COVERS      0.868     0.852     0.860        54\n",
      "                                 TELESCOPES      0.000     0.000     0.000         5\n",
      "                              WASTE_BASKETS      0.550     0.647     0.595        17\n",
      "         TELEVISION_MAIN_PLATE_REPLACEMENTS      0.852     0.885     0.868        78\n",
      "                       HABERDASHERY_RIBBONS      0.850     0.815     0.832       216\n",
      "                       POWERED_RIDE_ON_TOYS      0.898     0.919     0.908        86\n",
      "                                MONEY_BOXES      1.000     0.333     0.500         3\n",
      "                        LASER_PRINTER_DRUMS      0.933     0.966     0.949        29\n",
      "                RADIO_FREQUENCY_MICROPHONES      0.818     0.818     0.818        11\n",
      "                           FOOTBALL_JACKETS      0.636     0.483     0.549        29\n",
      "                        UNIVERSAL_HOME_GYMS      0.800     0.889     0.842        45\n",
      "                            SCALEXTRIC_CARS      0.818     0.600     0.692        15\n",
      "                        CONTINUOUS_LIGHTING      0.714     0.571     0.635        35\n",
      "                                 SHOE_RACKS      0.667     0.400     0.500         5\n",
      "                              DISHES_PLATES      0.787     0.814     0.800        59\n",
      "                           CIRCUIT_BREAKERS      0.927     0.933     0.930       149\n",
      "                                CAR_SCREENS      1.000     0.800     0.889        10\n",
      "                                 HAND_FILES      1.000     0.941     0.970        17\n",
      "                        FLATWARE_ORGANIZERS      0.200     0.167     0.182         6\n",
      "                               MIRROR_BALLS      0.828     0.857     0.842        28\n",
      "                                  KEYCHAINS      0.725     0.795     0.759        83\n",
      "                           TABLET_KEYBOARDS      1.000     0.500     0.667         4\n",
      "                            BABIES_FOOTWEAR      0.900     0.844     0.871        96\n",
      "                                      BOOTS      0.904     0.990     0.945       104\n",
      "                           LAPTOP_KEYBOARDS      0.897     0.963     0.929        27\n",
      "                              CURLING_IRONS      0.781     0.595     0.676        42\n",
      "                               EROTIC_BALLS      0.868     0.786     0.825        42\n",
      "                                 GYM_GLOVES      0.903     0.836     0.868        67\n",
      "                         LAPTOP_LCD_SCREENS      0.958     0.885     0.920        52\n",
      "                              KITCHEN_MOLDS      0.556     0.500     0.526        10\n",
      "                                    WINDOWS      0.955     0.913     0.933        23\n",
      "                                     LATHES      0.993     0.966     0.979       146\n",
      "                               BEER_FAUCETS      0.800     0.800     0.800         5\n",
      "                            WORKOUT_BENCHES      0.800     0.632     0.706        38\n",
      "                       MOTORCYCLE_BATTERIES      0.833     0.765     0.798        85\n",
      "                                FRAME_POOLS      0.796     0.907     0.848        43\n",
      "        AUDIO_AND_VIDEO_CABLES_AND_ADAPTERS      1.000     0.625     0.769         8\n",
      "                  ENGINE_COOLING_FAN_MOTORS      0.846     0.786     0.815        14\n",
      "                      EROTIC_MALE_UNDERWEAR      0.846     0.868     0.857        38\n",
      "                       HAND_AND_FOOT_CREAMS      0.935     0.867     0.900        83\n",
      "               PERSONAL_LUBRICANTS_AND_GELS      0.667     0.348     0.457        23\n",
      "                                 IDLER_ARMS      0.967     0.773     0.859        75\n",
      "                              PENIS_SLEEVES      0.727     0.889     0.800         9\n",
      "                        AUTOMOTIVE_ARMRESTS      0.811     0.782     0.796        55\n",
      "                               SEX_TOY_KITS      0.750     0.656     0.700        32\n",
      "                                 SAXOPHONES      0.970     0.941     0.955        34\n",
      "                           DECORATIVE_BOXES      0.794     0.871     0.831        31\n",
      "              ELECTRONIC_MUSCLE_STIMULATORS      0.605     0.684     0.642        38\n",
      "                             SEWING_THREADS      0.762     0.771     0.766        83\n",
      "                              JEWELRY_BOXES      0.000     0.000     0.000         6\n",
      "                                TRANSISTORS      0.717     0.611     0.660        54\n",
      "                        KIDS_WALKIE_TALKIES      0.818     0.750     0.783        12\n",
      "                                HAIR_DRYERS      0.875     0.933     0.903        15\n",
      "                                    ROUTERS      0.733     0.733     0.733        90\n",
      "                                    AIRBAGS      0.921     0.833     0.875        42\n",
      "                               SPORTS_CONES      0.829     1.000     0.906        29\n",
      "                                SPICE_RACKS      0.875     0.750     0.808        28\n",
      "                                    PENCILS      0.909     0.833     0.870        12\n",
      "                        CELLPHONE_BATTERIES      0.829     0.895     0.861        38\n",
      "                 CELLPHONE_REPAIR_TOOL_KITS      0.889     0.889     0.889        18\n",
      "                              PLAYING_CARDS      0.840     0.700     0.764        30\n",
      "                          VEHICLE_LED_BULBS      0.750     0.632     0.686        19\n",
      "                                     CHESTS      0.667     0.667     0.667         3\n",
      "                                 JUMP_ROPES      0.900     0.923     0.911        39\n",
      "                KIDS_TABLES_AND_CHAIRS_SETS      0.577     0.652     0.612        23\n",
      "                             CUTTING_BOARDS      1.000     0.167     0.286         6\n",
      "                          INTERCOOLER_HOSES      0.462     0.250     0.324        24\n",
      "                MOTORCYCLE_CHEST_PROTECTORS      0.600     0.429     0.500         7\n",
      "               BOX_SPRING_AND_MATTRESS_SETS      0.625     0.556     0.588         9\n",
      "                                  PLACEMATS      0.692     0.600     0.643        30\n",
      "                           NETWORK_SWITCHES      1.000     0.923     0.960        13\n",
      "                       HARD_DRIVES_AND_SSDS      0.909     0.952     0.930        42\n",
      "           INDUSTRIAL_AND_COMMERCIAL_SCALES      0.650     0.534     0.586        73\n",
      "                            ELECTRIC_GRILLS      0.708     0.810     0.756        21\n",
      "                       CAMERA_BATTERY_GRIPS      0.750     0.562     0.643        16\n",
      "                               TABLE_DRILLS      0.918     0.938     0.928        48\n",
      "                          CRIB_BEDDING_SETS      0.762     0.882     0.818       127\n",
      "                                  ORTHOTICS      0.857     0.750     0.800        16\n",
      "                            MEDICAL_WALKERS      0.900     0.900     0.900        10\n",
      "                              PILATES_BALLS      0.765     0.867     0.812        15\n",
      "                          BABY_SAFETY_LOCKS      0.889     0.976     0.930        41\n",
      "                          CAR_AC_CONDENSERS      1.000     0.969     0.984        96\n",
      "                                CAN_OPENERS      0.667     0.800     0.727        10\n",
      "                                  HEEL_CUPS      0.906     1.000     0.951        29\n",
      "                           EROTIC_MAGAZINES      0.929     0.812     0.867        16\n",
      "                        VEHICLE_BRAKE_DISCS      0.900     0.857     0.878        21\n",
      "                       FUEL_INJECTION_RAILS      0.782     0.672     0.723        64\n",
      "                                DOG_LEASHES      0.875     0.700     0.778        10\n",
      "                                  UMBRELLAS      0.952     0.882     0.916        68\n",
      "                 MICRODERMABRASION_MACHINES      0.800     0.667     0.727         6\n",
      "                             INSTANT_COFFEE      0.760     0.950     0.844        20\n",
      "                            ANIMAL_CLIPPERS      0.877     0.835     0.855        85\n",
      "                  AUTOMOTIVE_BUMPER_GRILLES      0.750     1.000     0.857         3\n",
      "                                   TRUMPETS      0.600     0.750     0.667         4\n",
      "                              DINING_CHAIRS      0.838     0.861     0.849        36\n",
      "                               CRASHED_CARS      0.893     0.944     0.918        71\n",
      "         ENGINE_CRANKSHAFT_POSITION_SENSORS      0.611     0.550     0.579        20\n",
      "                        INDUSTRIAL_BLENDERS      0.500     0.518     0.509        56\n",
      "                                 DOLLHOUSES      0.750     0.600     0.667         5\n",
      "                                ABS_SENSORS      0.964     0.950     0.957       140\n",
      "         AUTOMOTIVE_CLUTCH_MASTER_CYLINDERS      0.333     0.250     0.286         8\n",
      "                            MAKEUP_VANITIES      1.000     0.375     0.545         8\n",
      "                                 EGR_VALVES      0.833     0.714     0.769        21\n",
      "                  ENGINE_TAPPET_GUIDE_HOLDS      0.528     0.543     0.535        35\n",
      "                      MOTORCYCLE_RAIN_SUITS      0.942     0.942     0.942        52\n",
      "                                BEAUTY_WIGS      0.950     0.864     0.905        22\n",
      "                    EXTERNAL_LAPTOP_COOLERS      0.800     0.632     0.706        38\n",
      "                               SOLAR_PANELS      0.842     0.842     0.842        19\n",
      "                               CAR_SCANNERS      0.600     0.500     0.545         6\n",
      "                           ELECTRIC_BLOWERS      0.571     0.571     0.571         7\n",
      "                            PAPER_SHREDDERS      0.500     0.600     0.545         5\n",
      "                               STICKY_NOTES      0.975     0.907     0.940        43\n",
      "                     DESKTOP_COMPUTER_CASES      1.000     1.000     1.000         3\n",
      "                             SAFETY_GOGGLES      0.850     0.850     0.850        20\n",
      "                              RUBBER_FLOORS      0.846     0.846     0.846        13\n",
      "                                      KITES      1.000     0.600     0.750         5\n",
      "                                      VASES      0.542     0.351     0.426        37\n",
      "                               WORLD_GLOBES      1.000     1.000     1.000        23\n",
      "                               PUZZLE_CUBES      0.873     0.902     0.887        61\n",
      "                               BEDROOM_SETS      0.769     0.588     0.667        17\n",
      "                   AUTOMOTIVE_MIRROR_COVERS      1.000     0.500     0.667        12\n",
      "                                SNARE_DRUMS      1.000     0.667     0.800         6\n",
      "                ENGINE_COOLING_FAN_CLUTCHES      0.667     0.500     0.571         4\n",
      "  COLLECTIBLE_CANS_BOTTLES_AND_SODA_SIPHONS      0.880     0.733     0.800        30\n",
      "                            STRING_TRIMMERS      0.948     0.917     0.932        60\n",
      "                             SAFETY_HELMETS      0.833     0.714     0.769         7\n",
      "                              CERAMIC_TILES      0.727     0.615     0.667        13\n",
      "                                  BIRD_TOYS      0.800     0.783     0.791        46\n",
      "                                    TOILETS      0.885     0.852     0.868        27\n",
      "                             NOTEBOOK_CASES      0.778     0.824     0.800        17\n",
      "         COUNTERFEIT_MONEY_DETECTOR_MACHINE      0.500     0.500     0.500         2\n",
      "                            LAPTOP_HOUSINGS      1.000     0.833     0.909         6\n",
      "                    ORTHOPEDIC_WRIST_BRACES      0.846     0.857     0.852        77\n",
      "            POWER_STEERING_FLUID_RESERVOIRS      0.594     0.594     0.594        32\n",
      "                                      SAFES      0.822     0.771     0.796        48\n",
      "               TOY_GARAGES_AND_GAS_STATIONS      0.400     0.353     0.375        17\n",
      "                         MOTORCYCLE_JERSEYS      0.942     0.878     0.909        74\n",
      "                                 EQUALIZERS      0.688     0.733     0.710        15\n",
      "                    BLOOD_PRESSURE_MONITORS      0.897     0.867     0.881        30\n",
      "                        FITNESS_TRAMPOLINES      1.000     0.909     0.952        11\n",
      "                          IRRIGATION_VALVES      0.700     0.700     0.700        40\n",
      "                            BEER_DISPENSERS      0.778     0.583     0.667        24\n",
      "                        COMPRESSION_SLEEVES      1.000     0.333     0.500         3\n",
      "                                 TOY_PLANES      0.500     0.500     0.500         8\n",
      "                         BATHROOM_GRAB_BARS      0.962     0.962     0.962        26\n",
      "                              NETWORK_CARDS      0.692     0.581     0.632        31\n",
      "                                       RICE      1.000     0.667     0.800         3\n",
      "             STIMULATING_PILLS_AND_CAPSULES      0.568     0.658     0.610        38\n",
      "                               LABEL_MAKERS      0.609     0.609     0.609        23\n",
      "                                       CATS      0.667     0.526     0.588        19\n",
      "                                      WINES      0.916     0.956     0.935       182\n",
      "                             SECURITY_SEALS      0.808     0.913     0.857        23\n",
      "                              DEHUMIDIFIERS      0.571     0.444     0.500         9\n",
      "                                 COMPOSTERS      1.000     0.625     0.769         8\n",
      "                                       AXES      1.000     1.000     1.000         4\n",
      "                                   PADLOCKS      0.911     0.932     0.921        44\n",
      "                                   RACQUETS      0.000     0.000     0.000         6\n",
      "                            CAR_DOOR_HINGES      0.636     0.538     0.583        13\n",
      "                               BODY_SHAPERS      0.783     0.720     0.750        50\n",
      "                     MINI_COMPONENT_SYSTEMS      0.267     0.400     0.320        10\n",
      "                           INFLATABLE_SOFAS      1.000     0.857     0.923         7\n",
      "                        STATIONARY_BICYCLES      0.970     0.878     0.922        74\n",
      "             VEGETABLES_AND_FRUITS_CHOPPERS      0.583     0.778     0.667         9\n",
      "                                  VARNISHES      1.000     0.765     0.867        17\n",
      "                                   STYLUSES      0.949     0.881     0.914        42\n",
      "                                 LUNCHBOXES      0.900     0.818     0.857        22\n",
      "                 ENGINE_CYLINDER_HEAD_BOLTS      0.677     0.688     0.682        64\n",
      "                               BABY_BOTTLES      0.940     0.940     0.940       100\n",
      "              BICYCLE_AND_MOTORCYCLE_ALARMS      0.750     0.429     0.545         7\n",
      "                                 MDF_BOARDS      1.000     0.950     0.974        20\n",
      "                 BASEBALL_AND_SOFTBALL_BATS      1.000     1.000     1.000         5\n",
      "                                   DOG_BEDS      0.667     0.857     0.750         7\n",
      "                 LIQUID_HAND_AND_BODY_SOAPS      0.744     0.685     0.713        89\n",
      "                               VACUUM_TUBES      0.545     0.316     0.400        19\n",
      "                                  BABY_GYMS      0.750     0.500     0.600         6\n",
      "                                 CNC_LATHES      1.000     0.667     0.800         3\n",
      "                         MOTORCYCLE_FENDERS      0.722     0.619     0.667        21\n",
      "                                NIGHTSTANDS      0.821     0.793     0.807        29\n",
      "                                 LONGBOARDS      0.909     0.893     0.901        56\n",
      "                                PARTY_MASKS      0.846     0.786     0.815        70\n",
      "                           WASTE_CONTAINERS      1.000     0.667     0.800        12\n",
      "                             BABY_BODYSUITS      0.773     0.630     0.694        27\n",
      "                                      POUFS      0.600     0.600     0.600        10\n",
      "                             CUSHION_COVERS      0.923     0.947     0.935        76\n",
      "                               DRIVE_SHAFTS      0.792     0.792     0.792        24\n",
      "                     BRAKE_MASTER_CYLINDERS      0.889     0.667     0.762        12\n",
      "                          GAS_LIFT_SUPPORTS      0.857     0.923     0.889        39\n",
      "                           LAPTOP_BATTERIES      0.926     0.781     0.847        32\n",
      "                            POOL_WATERFALLS      1.000     0.923     0.960        13\n",
      "                                ECT_SENSORS      1.000     1.000     1.000         7\n",
      "                FOLDERS_AND_EXPANDING_FILES      0.818     0.643     0.720        28\n",
      "                            CAR_LIGHT_BULBS      0.700     0.500     0.583        14\n",
      "                              FISHING_VESTS      0.818     0.900     0.857        20\n",
      "                                SIDE_TABLES      0.944     0.919     0.932        37\n",
      "                         BARBECUE_TOOL_SETS      0.750     0.750     0.750        40\n",
      "                              DENTAL_CHAIRS      0.571     1.000     0.727         4\n",
      "                          SUNBATHING_CHAIRS      0.600     0.545     0.571        11\n",
      "                             BICYCLE_FRAMES      0.909     0.769     0.833        13\n",
      "                              IP_TELEPHONES      0.885     0.821     0.852        28\n",
      "                             DRINK_PITCHERS      0.875     0.667     0.757        21\n",
      "              AUTOMOTIVE_TRANSMISSION_GEARS      0.800     0.364     0.500        11\n",
      "                                MULTIMETERS      0.667     0.667     0.667         6\n",
      "                              MEAT_GRINDERS      0.000     0.000     0.000         3\n",
      "              INDUSTRIAL_ICE_CREAM_MACHINES      0.700     0.700     0.700        20\n",
      "                              FOOTBALL_CAPS      0.792     0.905     0.844        21\n",
      "                        LOAFERS_AND_OXFORDS      0.941     0.842     0.889        19\n",
      "                          PAINTBALL_O_RINGS      1.000     0.333     0.500         3\n",
      "                          ELECTRICAL_TIMERS      1.000     0.833     0.909         6\n",
      "                             TACTICAL_VESTS      0.909     0.769     0.833        13\n",
      "                            ACOUSTIC_PANELS      1.000     0.667     0.800         9\n",
      "                      ELECTRIC_HAND_PLANERS      0.750     0.429     0.545         7\n",
      "                              TOILETRY_BAGS      0.697     0.575     0.630        40\n",
      "                               WINE_CELLARS      0.909     0.909     0.909        11\n",
      "                ELECTRIC_DEMOLITION_HAMMERS      0.571     0.667     0.615        18\n",
      "                        TABLE_TENNIS_TABLES      1.000     0.667     0.800         6\n",
      "                                POOL_COVERS      0.909     0.909     0.909        22\n",
      "                                GARDEN_SOIL      0.857     0.667     0.750         9\n",
      "                  MARKING_AND_WARNING_TAPES      1.000     0.875     0.933        16\n",
      "                                    CYMBALS      1.000     0.500     0.667         8\n",
      "                      VIDEO_CAPTURE_DEVICES      0.778     0.656     0.712        32\n",
      "                           DRUM_BRAKE_SHOES      0.800     0.889     0.842         9\n",
      "                         DECORATIVE_BASKETS      0.433     0.295     0.351        44\n",
      "                                    CONDOMS      0.964     1.000     0.982        27\n",
      "                    TREADMILL_RUNNING_BELTS      1.000     1.000     1.000        13\n",
      "                          HAND_BRAKE_CABLES      0.966     1.000     0.982        28\n",
      "                                 SUNSCREENS      0.812     0.839     0.825        31\n",
      "                   MAGNETIC_WELDING_HOLDERS      1.000     0.833     0.909         6\n",
      "                       TOILET_PAPER_HOLDERS      0.842     0.640     0.727        25\n",
      "                      VIDEOCASSETTE_PLAYERS      0.750     0.750     0.750         4\n",
      "                CUT_OFF_AND_GRINDING_WHEELS      1.000     0.727     0.842        22\n",
      "                           JEWELRY_DISPLAYS      0.929     0.867     0.897        15\n",
      "                           CLEANING_SPONGES      1.000     1.000     1.000         8\n",
      "                            SKIN_REPELLENTS      0.900     0.818     0.857        11\n",
      "                                      SOCKS      0.900     0.910     0.905        89\n",
      "                 MERCHANDISER_REFRIGERATORS      0.250     0.125     0.167         8\n",
      "                               TENNIS_BALLS      1.000     0.667     0.800         3\n",
      "                           AIR_CONDITIONERS      0.870     0.870     0.870        23\n",
      "                              TOWEL_HOLDERS      0.800     0.889     0.842         9\n",
      "                                        RUM      1.000     1.000     1.000         4\n",
      "                                        GPS      0.714     0.556     0.625         9\n",
      "                                      HONEY      0.882     0.938     0.909        16\n",
      "                             MANUAL_HAMMERS      0.500     0.333     0.400         3\n",
      "                            BILLIARD_TABLES      0.833     0.833     0.833        18\n",
      "                             BICYCLE_WHEELS      1.000     0.167     0.286         6\n",
      "                               BRAKE_LIGHTS      0.933     0.824     0.875        17\n",
      "                             FETAL_DOPPLERS      1.000     0.714     0.833         7\n",
      "                                   LEGGINGS      0.273     0.100     0.146        30\n",
      "                               DIVING_MASKS      0.875     0.875     0.875         8\n",
      "                             LASER_POINTERS      1.000     0.750     0.857         8\n",
      "                          MOTORCYCLE_LEVERS      1.000     0.500     0.667         6\n",
      "                      PADDLE_TENNIS_RACKETS      1.000     0.889     0.941         9\n",
      "                            PORCELAIN_TILES      1.000     0.600     0.750         5\n",
      "                             BALL_PIT_BALLS      1.000     0.857     0.923         7\n",
      "                                 HARMONICAS      1.000     0.750     0.857         4\n",
      "                               PHOTO_ALBUMS      1.000     0.750     0.857         8\n",
      "                              LINGERIE_SETS      0.875     0.778     0.824         9\n",
      "                                 MEGAPHONES      1.000     0.778     0.875         9\n",
      "                         TV_REMOTE_CONTROLS      0.455     0.556     0.500         9\n",
      "                         TRADING_CARD_GAMES      0.806     0.644     0.716        45\n",
      "                            DRONE_BATTERIES      0.875     0.875     0.875         8\n",
      "                      UNIVERSAL_CAR_REMOTES      1.000     1.000     1.000         4\n",
      "                                DRUM_PEDALS      0.850     0.708     0.773        24\n",
      "                               TABLE_CLOCKS      0.333     0.250     0.286         8\n",
      "            EROTIC_ANAL_AND_VAGINAL_DOUCHES      0.500     0.333     0.400         3\n",
      "                         HAIR_STRAIGHTENERS      0.750     0.833     0.789        18\n",
      "                                 MICROWAVES      1.000     0.846     0.917        13\n",
      "                                    PUPPETS      0.000     0.000     0.000         4\n",
      "                           REFLECTIVE_VESTS      0.667     0.400     0.500         5\n",
      "                         EMBROIDERY_DESIGNS      1.000     0.812     0.897        16\n",
      "                                MICROMETERS      0.982     1.000     0.991        54\n",
      "                  MOTORCYCLE_IGNITION_COILS      0.871     0.931     0.900        29\n",
      "                                DINING_SETS      0.868     0.885     0.876        52\n",
      "                                 AIRBRUSHES      0.667     0.500     0.571         8\n",
      "                                 CARABINERS      1.000     0.667     0.800         9\n",
      "                               SOAP_HOLDERS      0.450     0.750     0.563        12\n",
      "                           INFLATABLE_POOLS      0.500     0.111     0.182         9\n",
      "                                 GIFT_CARDS      0.920     0.920     0.920        25\n",
      "                                POOL_LIGHTS      0.875     0.875     0.875        16\n",
      "                       PATIO_FURNITURE_SETS      0.600     0.750     0.667         8\n",
      "                                   SCOOTERS      0.857     0.500     0.632        12\n",
      "                            CYCLING_HELMETS      0.600     0.500     0.545         6\n",
      "                       CHOCOLATE_WATERFALLS      0.800     0.667     0.727         6\n",
      "                              HOSPITAL_BEDS      0.974     0.974     0.974        39\n",
      "                                MAP_SENSORS      0.714     0.588     0.645        17\n",
      "                               CLUTCH_FORKS      1.000     0.333     0.500         3\n",
      "                             ELBOW_SUPPORTS      0.556     0.556     0.556         9\n",
      "                          LAPTOP_BRIEFCASES      0.750     0.667     0.706         9\n",
      "                             OUTDOOR_TABLES      0.714     0.500     0.588        10\n",
      "                          DISTRIBUTION_KITS      0.778     0.500     0.609        14\n",
      "                                    JUICERS      0.000     0.000     0.000         5\n",
      "                           VEHICLE_TRACKERS      0.800     0.800     0.800         5\n",
      "                                BRAKE_DRUMS      1.000     1.000     1.000        10\n",
      "                           AB_ROLLER_WHEELS      1.000     1.000     1.000        10\n",
      "                                HOOD_HINGES      0.600     0.375     0.462         8\n",
      "                              DENTAL_PLIERS      0.889     0.706     0.787        34\n",
      "                            CLUTCH_BEARINGS      1.000     0.667     0.800         3\n",
      "                       POWER_STEERING_HOSES      0.750     0.500     0.600         6\n",
      "                                MOUTHWASHES      0.857     0.750     0.800         8\n",
      "                          WIRELESS_ANTENNAS      1.000     0.200     0.333         5\n",
      "                            LAUNDRY_BASKETS      1.000     0.667     0.800         6\n",
      "                           LIVING_ROOM_SETS      1.000     0.500     0.667        14\n",
      "                             BABY_PACIFIERS      1.000     0.667     0.800        15\n",
      "                             HAND_POLISHERS      0.600     0.429     0.500        14\n",
      "                               SHADE_CLOTHS      0.857     1.000     0.923        12\n",
      "                              HAMMER_DRILLS      0.500     0.625     0.556         8\n",
      "                             POWER_GRINDERS      0.714     0.385     0.500        13\n",
      "                              CAMERA_STRAPS      1.000     0.333     0.500         3\n",
      "                          LAWN_MOWER_BLADES      1.000     0.556     0.714         9\n",
      "                         PNEUMATIC_STAPLERS      1.000     1.000     1.000        13\n",
      "             THERMAL_REFRIGERATORS_AND_BAGS      0.810     0.607     0.694        28\n",
      "                               LIGHT_STANDS      0.750     0.429     0.545         7\n",
      "                                     STRAWS      1.000     0.750     0.857         8\n",
      "                                      SUITS      0.658     0.758     0.704        33\n",
      "                          BATHROOM_VANITIES      1.000     0.500     0.667         8\n",
      "                        VIBRATION_PLATFORMS      0.875     0.778     0.824        18\n",
      "                              DINING_TABLES      0.579     0.688     0.629        16\n",
      "                          TURNTABLE_NEEDLES      1.000     0.333     0.500         3\n",
      "                               HEARING_AIDS      1.000     0.700     0.824        10\n",
      "                    ORTHOPEDIC_WALKER_BOOTS      0.545     0.429     0.480        14\n",
      "                                        TEA      1.000     0.600     0.750        10\n",
      "                            TORQUE_WRENCHES      1.000     0.750     0.857         8\n",
      "                                CATS_LITTER      0.909     0.833     0.870        12\n",
      "                                MATE_GOURDS      0.800     0.727     0.762        11\n",
      "                                 MEAT_HOOKS      1.000     1.000     1.000         2\n",
      "                      AUTOMOTIVE_DEFLECTORS      1.000     1.000     1.000         5\n",
      "                  VARIABLE_FREQUENCY_DRIVES      1.000     0.857     0.923         7\n",
      "                                 SWEETENERS      1.000     0.875     0.933         8\n",
      "        AUTOMOTIVE_CELLPHONE_AND_GPS_MOUNTS      1.000     0.667     0.800         6\n",
      "                        RADIO_BASE_STATIONS      0.000     0.000     0.000         6\n",
      "                                   CRUTCHES      0.667     0.500     0.571         4\n",
      "                               POWER_STRIPS      1.000     0.500     0.667        10\n",
      "                              KATANA_SWORDS      0.900     0.692     0.783        13\n",
      "                       CATS_AND_DOGS_TREATS      0.750     0.429     0.545        14\n",
      "                        MEMORY_CARD_READERS      0.000     0.000     0.000         3\n",
      "                           TELEPHONE_CABLES      1.000     0.929     0.963        14\n",
      "                         SOLID_SWEET_PASTES      0.833     0.500     0.625        10\n",
      "                          DISPOSABLE_GLOVES      1.000     0.333     0.500         3\n",
      "                       MOTORCYCLE_GRAB_BARS      0.667     0.667     0.667         3\n",
      "                             PLUNGE_ROUTERS      0.400     0.250     0.308         8\n",
      "                        GROOVE_JOINT_PLIERS      0.800     1.000     0.889         4\n",
      "                         BICYCLE_HANDLEBARS      0.333     0.200     0.250         5\n",
      "                         SOLDERING_STATIONS      0.833     0.769     0.800        26\n",
      "                              TANDEM_CHAIRS      0.429     0.429     0.429         7\n",
      "                               GARAGE_DOORS      0.417     0.417     0.417        12\n",
      "                        SPARK_PLUG_WIRESETS      0.926     0.781     0.847        32\n",
      "                      ELECTRIC_SHOWER_HEADS      0.500     0.200     0.286         5\n",
      "                       PORTABLE_DVD_PLAYERS      0.500     0.222     0.308         9\n",
      "                                     DILDOS      0.455     0.625     0.526         8\n",
      "                  INDUSTRIAL_DOUGH_KNEADERS      0.636     0.538     0.583        13\n",
      "                  UNIVERSAL_REMOTE_CONTROLS      1.000     0.333     0.500         3\n",
      "                                WHEEL_STUDS      0.333     0.333     0.333         6\n",
      "                           FABRIC_SOFTENERS      0.944     0.895     0.919        19\n",
      "             MOTORCYCLE_DISTRIBUTION_CHAINS      0.842     0.842     0.842        19\n",
      "                      DOOR_AND_WINDOW_LOCKS      0.500     0.333     0.400         3\n",
      "                                       GINS      1.000     0.250     0.400         4\n",
      "                                HAND_TRUCKS      1.000     0.250     0.400         4\n",
      "                                   STAPLERS      1.000     0.667     0.800         6\n",
      "                                  APERITIFS      0.714     1.000     0.833         5\n",
      "                            SHOWER_CURTAINS      1.000     1.000     1.000         4\n",
      "                             ANTIQUE_CHAIRS      0.333     0.143     0.200         7\n",
      "                              YOGURT_MAKERS      1.000     0.400     0.571         5\n",
      "                                SHIN_GUARDS      0.000     0.000     0.000         2\n",
      "                            GOLF_CLUBS_SETS      0.875     0.778     0.824         9\n",
      "                 FOOTBALL_GOALKEEPER_GLOVES      1.000     0.812     0.897        16\n",
      "                               LASER_LEVELS      0.667     0.500     0.571         8\n",
      "                       KEYBOARD_CONTROLLERS      0.167     0.200     0.182         5\n",
      "                         ALTERNATOR_PULLEYS      0.944     1.000     0.971        17\n",
      "           CAMERA_AND_CELLPHONE_STABILIZERS      0.667     0.667     0.667         3\n",
      "                               BABY_JUMPERS      1.000     0.500     0.667         6\n",
      "                CAMERA_REPLACEMENT_DISPLAYS      0.500     0.857     0.632         7\n",
      "                                CRANKSHAFTS      0.800     0.571     0.667        21\n",
      "                               BREAD_MAKERS      0.600     1.000     0.750         3\n",
      "                   SCHOOL_AND_OFFICE_PAPERS      1.000     0.375     0.545         8\n",
      "                   STOVETOP_POPCORN_POPPERS      0.667     0.750     0.706         8\n",
      "                     BREAST_FEEDING_PILLOWS      0.889     0.727     0.800        11\n",
      "                              POOL_CLEANERS      1.000     0.600     0.750         5\n",
      "                             LINEMAN_PLIERS      1.000     1.000     1.000         3\n",
      "                                 COAT_RACKS      1.000     0.167     0.286         6\n",
      "                                 POOL_PUMPS      1.000     0.750     0.857        12\n",
      "                    WATER_PURIFIERS_FILTERS      0.529     0.450     0.486        20\n",
      "                              SAFETY_GLOVES      1.000     0.333     0.500         6\n",
      "                         ISOPROPYL_ALCOHOLS      1.000     0.778     0.875         9\n",
      "              VEHICLE_BRAKE_HYDRAULIC_HOSES      0.750     0.333     0.462         9\n",
      "                             MEDICINE_BALLS      1.000     0.500     0.667         4\n",
      "                            KITCHEN_GRATERS      0.000     0.000     0.000         5\n",
      "                             NAPKIN_HOLDERS      0.800     0.444     0.571         9\n",
      "                              PUNCHING_BAGS      0.000     0.000     0.000         3\n",
      "                                ESPADRILLES      0.500     0.500     0.500         8\n",
      "                        CAR_CENTER_CONSOLES      0.500     0.500     0.500         6\n",
      "                            VIDEO_CASSETTES      0.600     1.000     0.750         6\n",
      "                             POTENTIOMETERS      1.000     0.400     0.571         5\n",
      "                   READY_TO_DRINK_COCKTAILS      0.333     0.500     0.400         2\n",
      "                                FLOOR_LAMPS      0.700     0.636     0.667        11\n",
      "                           DRONE_PROPELLERS      0.500     0.167     0.250         6\n",
      "                                      TENTS      0.750     0.818     0.783        11\n",
      "                           SAFETY_HARNESSES      0.250     0.250     0.250         4\n",
      "                                   SYRINGES      0.500     0.500     0.500         4\n",
      "                                  BEDLINERS      0.857     0.750     0.800        16\n",
      "                    ELECTROLYTIC_CAPACITORS      0.667     0.571     0.615         7\n",
      "                               BASKET_BALLS      1.000     1.000     1.000         2\n",
      "                                  OTOSCOPES      0.200     0.250     0.222         4\n",
      "                              WAFFLE_MAKERS      0.750     0.750     0.750         8\n",
      "                                PENIS_RINGS      1.000     0.250     0.400         4\n",
      "                         ELECTRIC_AIR_PUMPS      0.875     0.875     0.875         8\n",
      "                            COFFEE_CAPSULES      0.500     0.400     0.444         5\n",
      "                        BABY_PACIFIER_CLIPS      0.625     1.000     0.769         5\n",
      "                                  KEY_RACKS      0.000     0.000     0.000         4\n",
      "                                 DEODORANTS      0.800     0.800     0.800         5\n",
      "                         INDUSTRIAL_PULLEYS      0.625     1.000     0.769         5\n",
      "                              BILL_COUNTERS      1.000     1.000     1.000         5\n",
      "                 AUDIO_AND_VIDEO_CONNECTORS      0.000     0.000     0.000         4\n",
      "                           WASHING_MACHINES      0.857     0.500     0.632        12\n",
      "                ENGINE_COOLING_FAN_SWITCHES      0.750     0.500     0.600         6\n",
      "                       AUTOMOTIVE_BATTERIES      0.333     0.200     0.250         5\n",
      "                       CHIP_AND_DIP_SERVERS      0.000     0.000     0.000         4\n",
      "                               ARCHERY_BOWS      0.857     1.000     0.923         6\n",
      "                            VINYL_FLOORINGS      0.667     0.571     0.615         7\n",
      "                             MENSTRUAL_CUPS      1.000     1.000     1.000         5\n",
      "                              SLEEPING_BAGS      0.500     0.500     0.500         4\n",
      "                              RUBBER_STAMPS      1.000     0.800     0.889         5\n",
      "                             DRYER_MACHINES      0.750     0.750     0.750         8\n",
      "                           ENERGETIC_STONES      0.500     0.429     0.462         7\n",
      "                             BICYCLE_PEDALS      1.000     0.667     0.800         9\n",
      "                     MOTORCYCLE_CARBURETORS      1.000     0.200     0.333         5\n",
      "                          SCREWDRIVERS_SETS      0.000     0.000     0.000         3\n",
      "                               EDIBLE_SEEDS      0.250     0.667     0.364         3\n",
      "                       STORE_SHOPPING_CARTS      1.000     0.750     0.857         4\n",
      "                        SWIMMING_NOSE_CLIPS      1.000     0.500     0.667         2\n",
      "                                AFTERSHAVES      0.000     0.000     0.000         3\n",
      "                             AIRBAG_MODULES      1.000     1.000     1.000         3\n",
      "                                  HAND_SAWS      0.800     0.667     0.727         6\n",
      "          NECK_GAITERS_MASKS_AND_BALACLAVAS      1.000     0.500     0.667         2\n",
      "                         MAGNETIC_COMPASSES      0.667     0.333     0.444         6\n",
      "                         MAKEUP_TRAIN_CASES      1.000     0.500     0.667         4\n",
      "                           POPCORN_MACHINES      1.000     0.889     0.941         9\n",
      "                               LAMP_HOLDERS      0.500     0.500     0.500         4\n",
      "                   IGNITION_CONTROL_MODULES      0.500     0.286     0.364         7\n",
      "                            CAR_FRONT_MASKS      1.000     0.500     0.667         2\n",
      "                              HAND_BLENDERS      0.000     0.000     0.000         3\n",
      "                             KIDS_TRICYCLES      1.000     0.750     0.857         4\n",
      "                             AIRGUN_PELLETS      0.500     0.250     0.333         4\n",
      "                           AUTOMOTIVE_SEATS      0.500     0.333     0.400         6\n",
      "             MOTORCYCLE_TRANSMISSION_CROWNS      1.000     0.250     0.400         4\n",
      "                                 LAMINATORS      0.083     0.143     0.105         7\n",
      "                                CEREAL_BARS      1.000     0.667     0.800         3\n",
      "                               MUSIC_ALBUMS      0.000     0.000     0.000         4\n",
      "                  AUTOMOTIVE_CV_JOINT_BOOTS      0.667     0.571     0.615         7\n",
      "                          MICROWAVE_KEYPADS      1.000     1.000     1.000         3\n",
      "                          WALL_ANCHOR_PLUGS      0.667     0.500     0.571         4\n",
      "                                DRUM_STANDS      0.500     0.429     0.462         7\n",
      "                                PET_COLLARS      0.769     0.714     0.741        14\n",
      "                            GATE_GEAR_RACKS      1.000     0.500     0.667         4\n",
      "                                  CAR_HOODS      1.000     0.667     0.800         3\n",
      "                       SCREEN_PRINTING_KITS      1.000     0.750     0.857         4\n",
      "                                 LED_STRIPS      1.000     0.750     0.857         4\n",
      "                            SANDWICH_MAKERS      0.000     0.000     0.000         2\n",
      "                             DENTAL_FLOSSES      1.000     0.500     0.667         2\n",
      "                             CAMERA_FLASHES      1.000     0.800     0.889         5\n",
      "                          DOG_NAIL_CLIPPERS      0.000     0.000     0.000         2\n",
      "                                   PINBALLS      1.000     0.667     0.800         6\n",
      "                                     GAUZES      0.571     0.667     0.615         6\n",
      "                                       SODS      0.667     0.667     0.667         3\n",
      "                         ELECTRICITY_METERS      0.000     0.000     0.000         4\n",
      "                            METAL_DETECTORS      1.000     0.333     0.500         3\n",
      "                         ELECTRIC_CHAINSAWS      1.000     0.333     0.500         3\n",
      "                           KNITTING_NEEDLES      0.444     0.667     0.533         6\n",
      "                          SWIMMING_EARPLUGS      0.000     0.000     0.000         2\n",
      "                                SOUND_CARDS      1.000     0.600     0.750         5\n",
      "                                   STEPPERS      0.000     0.000     0.000         4\n",
      "                                FANNY_PACKS      0.000     0.000     0.000         4\n",
      "                         TOOTHBRUSH_HOLDERS      0.000     0.000     0.000         2\n",
      "                         TABLE_TENNIS_BALLS      1.000     1.000     1.000         2\n",
      "                ENGINE_OIL_PRESSURE_SENSORS      1.000     0.333     0.500         3\n",
      "                                ICE_BUCKETS      0.800     0.571     0.667         7\n",
      "                              MASSAGE_SOFAS      1.000     0.500     0.667         4\n",
      "                             STYLING_CHAIRS      1.000     0.500     0.667         2\n",
      "                              BICYCLE_SEATS      1.000     0.667     0.800         3\n",
      "                           VOLLEYBALL_BALLS      0.000     0.000     0.000         3\n",
      "                          SPHYGMOMANOMETERS      0.667     0.500     0.571         4\n",
      "                             BINDING_SPINES      0.000     0.000     0.000         2\n",
      "                   DIGITAL_WEATHER_STATIONS      0.000     0.000     0.000         2\n",
      "                         HOME_BOTTLE_STANDS      0.667     0.667     0.667         3\n",
      "                                  DOORBELLS      0.000     0.000     0.000         2\n",
      "                               DRIED_FRUITS      0.167     0.500     0.250         2\n",
      "                           BOXING_HEADGEARS      1.000     0.500     0.667         2\n",
      "\n",
      "                                   accuracy                          0.905    138550\n",
      "                                  macro avg      0.820     0.757     0.777    138550\n",
      "                               weighted avg      0.906     0.905     0.905    138550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(last_saved_model, dataset, batch_size, labels_dict, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
