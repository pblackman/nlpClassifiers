{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from os.path import join\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from nlpClassifiers.data.dataset  import NLPDataset, Vocabulary\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from scipy.special import expit\n",
    "import gensim\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '../../'\n",
    "PATH_TO_VIRTUAL_OPERATOR_DATA = join(ROOT, \"data/virtual-operator\")\n",
    "PATH_TO_AGENT_BENCHMARK_DATA = join(ROOT, \"data/agent-benchmark\")\n",
    "PATH_TO_ML_PT_DATA = join(ROOT, \"data/mercado-livre-pt-only\")\n",
    "\n",
    "PATH_TO_VIRTUAL_OPERATOR_MODELS = join(ROOT, \"models/virtual-operator\")\n",
    "PATH_TO_AGENT_BENCHMARK_MODELS = join(ROOT, \"models/agent-benchmark\")\n",
    "PATH_TO_ML_PT_MODELS = join(ROOT, \"models/mercado-livre-pt-only\")\n",
    "EMBEDDINGS_FILE = '../../data/word2vec/cbow_s300_en.txt'\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_WORDS = 30000\n",
    "dataset = 'agent-benchmark'\n",
    "\n",
    "sentence_max_len = 82\n",
    "batch_size = 512\n",
    "biLSTM = True\n",
    "epochs=30\n",
    "stopwords_lang = None\n",
    "gpu=1\n",
    "lr=1e-3\n",
    "clipping_value=0.25\n",
    "save_name='bilstm-pytorch-word2vec-public-en'\n",
    "patience=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_TO_MODELS = {\"virtual-operator\": PATH_TO_VIRTUAL_OPERATOR_MODELS, \"agent-benchmark\": PATH_TO_AGENT_BENCHMARK_MODELS, \"mercado-livre-pt\": PATH_TO_ML_PT_MODELS}\n",
    "FULL_PATH_TO_MODELS = join(BASE_PATH_TO_MODELS[dataset], \"bilstm-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset, subset):\n",
    "    BASE_PATH_TO_DATASET = {\"virtual-operator\": PATH_TO_VIRTUAL_OPERATOR_DATA, \"agent-benchmark\": PATH_TO_AGENT_BENCHMARK_DATA, \"mercado-livre-pt\": PATH_TO_ML_PT_DATA}\n",
    "    BASE_PATH_TO_DATASET = {\"train\": join(BASE_PATH_TO_DATASET[dataset], \"train.csv\"), \"val\": join(BASE_PATH_TO_DATASET[dataset], \"val.csv\"), \"test\": join(BASE_PATH_TO_DATASET[dataset], \"test.csv\")}\n",
    "    FULL_PATH_TO_DATASET = BASE_PATH_TO_DATASET[subset]\n",
    "    \n",
    "    if dataset == \"mercado-livre-pt\":\n",
    "        sep=\",\"\n",
    "    else:\n",
    "        sep=\";\"\n",
    "    data = pd.read_csv(FULL_PATH_TO_DATASET, sep=sep, names =['utterance','label'], header=None, dtype={'utterance':str, 'label': str} )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = read_data(dataset, \"train\")\n",
    "val_df = read_data(dataset, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322 tokens not found in vocabulary.\n"
     ]
    }
   ],
   "source": [
    "voc = Vocabulary('CNN', stopwords_lang)\n",
    "voc.build_vocab(train_df['utterance'].tolist() +  val_df['utterance'].tolist(), MAX_WORDS)\n",
    "embedding_weights = voc.load_embeddings(EMBEDDINGS_FILE, EMBEDDING_DIM)\n",
    "if voc.num_words < MAX_WORDS or MAX_WORDS == 0:\n",
    "    MAX_WORDS = voc.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7370, 300])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = NLPDataset(dataset, \"train\", sentence_max_len, vocab= voc)\n",
    "labels_dict = train_corpus.labels_dict\n",
    "val_corpus = NLPDataset(dataset, \"val\", sentence_max_len, labels_dict = labels_dict, vocab= voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train_corpus,\n",
    "            sampler = RandomSampler(train_corpus),\n",
    "            batch_size = batch_size,\n",
    "            pin_memory=True,\n",
    "            #shuffle=True,\n",
    "            worker_init_fn=_init_fn,\n",
    "            num_workers=0\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_corpus,\n",
    "            sampler = RandomSampler(val_corpus),\n",
    "            batch_size = batch_size,\n",
    "            pin_memory=True,\n",
    "            #shuffle=True,\n",
    "            worker_init_fn=_init_fn,\n",
    "            num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=10):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, device, num_classes, num_features, criterion, embedding_dim, bidirectional, embedding_weights=None):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = embedding_dim\n",
    "        self.lstm_act = nn.Tanh()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.device = device\n",
    "        self.n_layers = 1\n",
    "        self.embedding = nn.Embedding(num_features, embedding_dim, padding_idx = 0)\n",
    "        \n",
    "        if(embedding_weights is not None):\n",
    "            print(\"Embedding layer Weights won't be updated.\")\n",
    "            #self.embedding.from_pretrained(embedding_weights, freeze=True)\n",
    "            self.embedding.load_state_dict({'weight': embedding_weights})\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        else:\n",
    "            self.embedding.weight.data = torch.zeros(self.embedding.weight.data.size())\n",
    "            self.embedding.weight.requires_grad = True\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.lstm_units, num_layers=self.n_layers, bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)       \n",
    "        self.linear = nn.Linear(self.num_directions * self.lstm_units, num_classes)   \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "\n",
    "        # lstm step => then ONLY take the sequence's final timetep to pass into the linear/dense layer\n",
    "        # Note: lstm_out contains outputs for every step of the sequence we are looping over (for BPTT)\n",
    "        # but we just need the output of the last step of the sequence, aka lstm_out[-1]\n",
    "        #print(\"x input:\", x.size())\n",
    "        #set_trace()\n",
    "        x = self.embedding(x)\n",
    "        #print(\"embeddings:\", x.size())\n",
    "        x_, (h_n, c_n) = self.lstm(x)\n",
    "        h = x_[:,-0,:]\n",
    "        #h = self.lstm_act(h)\n",
    "        '''        \n",
    "        if self.num_directions == 2:\n",
    "            h = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            h = h_n[-1, :, :]\n",
    "        '''\n",
    "        #print(\"h before activation:\", h.size())\n",
    "        #print(\"h after activation:\", h.size())\n",
    "        output = self.dropout(h)\n",
    "        output = self.linear(output)\n",
    "        #print(\"output after linear:\", output.size())\n",
    "        loss = self.criterion(output.squeeze(), y)\n",
    "        #print(\"loss:\", loss.size())\n",
    "        return loss,output\n",
    "    \n",
    "    def init_weights(self):  \n",
    "        nn.init.xavier_normal_(self.lstm.weight_hh_l0)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_ih_l0)\n",
    "        nn.init.xavier_normal_(self.linear.weight.data)\n",
    "       \n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer Weights won't be updated.\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = LSTMNet(device, train_corpus.num_labels, MAX_WORDS, criterion, EMBEDDING_DIM, biLSTM, embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optimizer = Adam(model.parameters(),lr, betas=(0.7, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_clone = model.embedding.weight.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(embeddings_clone,model.embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0801,  0.1050,  0.0498,  ...,  0.0037,  0.0476, -0.0688],\n",
       "        [ 0.0801,  0.1050,  0.0498,  ...,  0.0037,  0.0476, -0.0688],\n",
       "        [ 0.0070, -0.0732,  0.1719,  ...,  0.0112,  0.1641,  0.1069],\n",
       "        ...,\n",
       "        [ 0.1221,  0.2578,  0.0317,  ...,  0.0918,  0.1377, -0.1348],\n",
       "        [ 0.1221,  0.2578,  0.0317,  ...,  0.0918,  0.1377, -0.1348],\n",
       "        [-0.1147, -0.0835, -0.1484,  ..., -0.1279,  0.0330,  0.1108]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    acc = (labels.cpu() == logits.cpu().argmax(-1)).float().detach().numpy()\n",
    "    return float(100 * acc.sum() / len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 3.41\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 39.84\n",
      "Batch accuracy: 38.87\n",
      "Batch accuracy: 41.02\n",
      "Batch accuracy: 42.47\n",
      "  Accuracy: 40.55\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 2 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 2.36\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 54.30\n",
      "Batch accuracy: 59.57\n",
      "Batch accuracy: 58.01\n",
      "Batch accuracy: 55.38\n",
      "  Accuracy: 56.81\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 3 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.78\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 61.91\n",
      "Batch accuracy: 65.04\n",
      "Batch accuracy: 64.06\n",
      "Batch accuracy: 64.19\n",
      "  Accuracy: 63.80\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 4 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.50\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 65.62\n",
      "Batch accuracy: 65.43\n",
      "Batch accuracy: 67.97\n",
      "Batch accuracy: 66.93\n",
      "  Accuracy: 66.49\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 5 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.29\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 68.95\n",
      "Batch accuracy: 67.77\n",
      "Batch accuracy: 68.16\n",
      "Batch accuracy: 69.08\n",
      "  Accuracy: 68.49\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 6 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.14\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 70.90\n",
      "Batch accuracy: 68.16\n",
      "Batch accuracy: 70.70\n",
      "Batch accuracy: 67.71\n",
      "  Accuracy: 69.37\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 7 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.04\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 71.29\n",
      "Batch accuracy: 71.48\n",
      "Batch accuracy: 73.05\n",
      "Batch accuracy: 69.47\n",
      "  Accuracy: 71.32\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 8 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.98\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 71.48\n",
      "Batch accuracy: 69.92\n",
      "Batch accuracy: 69.73\n",
      "Batch accuracy: 69.67\n",
      "  Accuracy: 70.20\n",
      "The model does not improve for 1 epochs!\n",
      "\n",
      "======== Epoch 9 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.91\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 74.22\n",
      "Batch accuracy: 73.44\n",
      "Batch accuracy: 74.61\n",
      "Batch accuracy: 70.65\n",
      "  Accuracy: 73.23\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 10 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.85\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 74.22\n",
      "Batch accuracy: 73.24\n",
      "Batch accuracy: 72.85\n",
      "Batch accuracy: 68.30\n",
      "  Accuracy: 72.15\n",
      "The model does not improve for 1 epochs!\n",
      "\n",
      "======== Epoch 11 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.80\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 74.61\n",
      "Batch accuracy: 75.00\n",
      "Batch accuracy: 73.05\n",
      "Batch accuracy: 72.41\n",
      "  Accuracy: 73.77\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 12 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.75\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 72.46\n",
      "Batch accuracy: 72.85\n",
      "Batch accuracy: 76.95\n",
      "Batch accuracy: 76.52\n",
      "  Accuracy: 74.70\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 13 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.71\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 72.66\n",
      "Batch accuracy: 74.41\n",
      "Batch accuracy: 77.34\n",
      "Batch accuracy: 71.82\n",
      "  Accuracy: 74.06\n",
      "The model does not improve for 1 epochs!\n",
      "\n",
      "======== Epoch 14 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.68\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 70.12\n",
      "Batch accuracy: 74.80\n",
      "Batch accuracy: 69.92\n",
      "Batch accuracy: 72.02\n",
      "  Accuracy: 71.71\n",
      "The model does not improve for 2 epochs!\n",
      "\n",
      "======== Epoch 15 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.63\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 75.39\n",
      "Batch accuracy: 74.22\n",
      "Batch accuracy: 75.00\n",
      "Batch accuracy: 74.36\n",
      "  Accuracy: 74.74\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 16 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.61\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 71.68\n",
      "Batch accuracy: 74.22\n",
      "Batch accuracy: 74.22\n",
      "Batch accuracy: 77.10\n",
      "  Accuracy: 74.31\n",
      "The model does not improve for 1 epochs!\n",
      "\n",
      "======== Epoch 17 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.57\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 72.27\n",
      "Batch accuracy: 73.24\n",
      "Batch accuracy: 72.07\n",
      "Batch accuracy: 76.32\n",
      "  Accuracy: 73.47\n",
      "The model does not improve for 2 epochs!\n",
      "\n",
      "======== Epoch 18 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.53\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 75.39\n",
      "Batch accuracy: 75.59\n",
      "Batch accuracy: 74.80\n",
      "Batch accuracy: 74.36\n",
      "  Accuracy: 75.04\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 19 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.52\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 74.41\n",
      "Batch accuracy: 73.63\n",
      "Batch accuracy: 75.59\n",
      "Batch accuracy: 73.97\n",
      "  Accuracy: 74.40\n",
      "The model does not improve for 1 epochs!\n",
      "\n",
      "======== Epoch 20 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.49\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 74.80\n",
      "Batch accuracy: 74.61\n",
      "Batch accuracy: 73.83\n",
      "Batch accuracy: 73.78\n",
      "  Accuracy: 74.25\n",
      "The model does not improve for 2 epochs!\n",
      "\n",
      "======== Epoch 21 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.47\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 73.83\n",
      "Batch accuracy: 74.02\n",
      "Batch accuracy: 74.22\n",
      "Batch accuracy: 76.13\n",
      "  Accuracy: 74.55\n",
      "The model does not improve for 3 epochs!\n",
      "\n",
      "======== Epoch 22 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.43\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 70.51\n",
      "Batch accuracy: 77.73\n",
      "Batch accuracy: 78.32\n",
      "Batch accuracy: 74.76\n",
      "  Accuracy: 75.33\n",
      "New best model, saving it!\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\n",
      "======== Epoch 23 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.37\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 75.78\n",
      "Batch accuracy: 74.80\n",
      "Batch accuracy: 74.02\n",
      "Batch accuracy: 76.71\n",
      "  Accuracy: 75.33\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 24 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.35\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 76.17\n",
      "Batch accuracy: 74.22\n",
      "Batch accuracy: 77.73\n",
      "Batch accuracy: 74.17\n",
      "  Accuracy: 75.57\n",
      "New best model, saving it!\n",
      "\n",
      "======== Epoch 25 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.35\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 76.95\n",
      "Batch accuracy: 74.61\n",
      "Batch accuracy: 75.78\n",
      "Batch accuracy: 74.36\n",
      "  Accuracy: 75.43\n",
      "The model does not improve for 1 epochs!\n",
      "\n",
      "======== Epoch 26 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.34\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 76.95\n",
      "Batch accuracy: 72.66\n",
      "Batch accuracy: 76.37\n",
      "Batch accuracy: 76.32\n",
      "  Accuracy: 75.57\n",
      "New best model, saving it!\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
      "\n",
      "======== Epoch 27 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.33\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 74.22\n",
      "Batch accuracy: 75.20\n",
      "Batch accuracy: 75.39\n",
      "Batch accuracy: 76.91\n",
      "  Accuracy: 75.43\n",
      "The model does not improve for 1 epochs!\n",
      "\n",
      "======== Epoch 28 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.33\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 74.80\n",
      "Batch accuracy: 74.80\n",
      "Batch accuracy: 77.15\n",
      "Batch accuracy: 75.15\n",
      "  Accuracy: 75.48\n",
      "The model does not improve for 2 epochs!\n",
      "\n",
      "======== Epoch 29 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.33\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 76.37\n",
      "Batch accuracy: 75.20\n",
      "Batch accuracy: 75.39\n",
      "Batch accuracy: 73.78\n",
      "  Accuracy: 75.18\n",
      "The model does not improve for 3 epochs!\n",
      "\n",
      "======== Epoch 30 / 30 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.33\n",
      "\n",
      "Running Validation...\n",
      "Batch accuracy: 76.95\n",
      "Batch accuracy: 74.80\n",
      "Batch accuracy: 73.83\n",
      "Batch accuracy: 75.15\n",
      "  Accuracy: 75.18\n",
      "The model does not improve for 4 epochs!\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-06.\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3,verbose=True, factor=0.1)\n",
    "\n",
    "best_epoch = -1\n",
    "last_saved_model = \"\"\n",
    "training_stats = []\n",
    "global_step = 0\n",
    "best_val_acc = float(\"-inf\")\n",
    "best_model_wts = None\n",
    "best_curr_val = 0\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "         # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    .'.format(step, len(train_dataloader)))\n",
    "        b_input_ids = batch[0]\n",
    "        b_labels = batch[1].clone().detach().to(device)\n",
    "        b_input_ids = b_input_ids.clone().detach().to(device).long()\n",
    "        loss, logits= model(b_input_ids, b_labels)\n",
    "        step_loss = loss.item()\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print(\"  step loss: {0:.2f}\".format(step_loss))\n",
    "        total_train_loss += step_loss\n",
    "        global_step += 1\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    # Measure how long this epoch took.\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0.0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_ids = b_input_ids.clone().detach().to(device).long()\n",
    "        b_labels = batch[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            loss, logits = model(b_input_ids, b_labels)\n",
    "        total_eval_loss += loss\n",
    "        batch_acc = get_accuracy_from_logits(logits, b_labels)\n",
    "        print(\"Batch accuracy: {0:.2f}\".format(batch_acc))\n",
    "        total_eval_accuracy += batch_acc\n",
    "    \n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    \n",
    "    if avg_val_accuracy > best_val_acc:\n",
    "        print(f\"New best model, saving it!\")\n",
    "        if avg_val_accuracy > best_curr_val:\n",
    "            best_curr_val = avg_val_accuracy\n",
    "        if last_saved_model:\n",
    "            shutil.rmtree(last_saved_model)\n",
    "        model_path = Path(\n",
    "            FULL_PATH_TO_MODELS,\n",
    "            f\"base-dataset-{dataset}-{save_name}\"\n",
    "        )\n",
    "        last_saved_model = model_path\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "        best_val_acc = avg_val_accuracy\n",
    "        torch.save(model, join(model_path, \"best-model.pth\"))\n",
    "        best_epoch = epoch_i\n",
    "        n_epochs_no_improvement = 0\n",
    "    elif avg_val_accuracy > best_curr_val:\n",
    "        best_curr_val = avg_val_accuracy\n",
    "        n_epochs_no_improvement = 0\n",
    "    else:\n",
    "        n_epochs_no_improvement += 1\n",
    "        print(f\"The model does not improve for {n_epochs_no_improvement} epochs!\")\n",
    "\n",
    "    if n_epochs_no_improvement > patience:\n",
    "        print(f\"====>Stopping training, the model did not improve for {n_epochs_no_improvement}\\n====>Best epoch: {best_epoch + 1}.\")\n",
    "        break\n",
    "    scheduler.step(total_eval_loss)\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model_path: Path,\n",
    "    dataset: str,\n",
    "    batch_size: int,\n",
    "    labels_dict,\n",
    "    device: torch.device\n",
    "):\n",
    "\n",
    "    print(f\"====Loading dataset for testing\")\n",
    "    test_corpus = NLPDataset(dataset, \"test\", sentence_max_len, labels_dict = labels_dict, vocab= voc)\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_corpus,\n",
    "        batch_size=batch_size,\n",
    "        #sampler = RandomSampler(test_corpus),\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    print(f\"====Loading model for testing\")\n",
    "    model = torch.load(join(model_path, \"best-model.pth\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pred_labels = []\n",
    "    test_labels = []\n",
    "    logits_list = []\n",
    "\n",
    "    def _list_from_tensor(tensor):\n",
    "        if tensor.numel() == 1:\n",
    "            return [tensor.item()]\n",
    "        return list(tensor.cpu().detach().numpy())\n",
    "\n",
    "    print(\"====Testing model...\")\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        b_labels = batch[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(b_input_ids, b_labels)\n",
    "            preds = np.argmax(logits.cpu(), axis=1) # Convert one-hot to index\n",
    "            b_labels = b_labels.int()\n",
    "            pred_labels.extend(_list_from_tensor(preds))\n",
    "            test_labels.extend(_list_from_tensor(b_labels))\n",
    "        logits_list.extend(_list_from_tensor(logits))\n",
    "\n",
    "    print(classification_report(test_labels, pred_labels, labels=list(labels_dict.values()), target_names=np.array(list(labels_dict.keys())), digits=3, output_dict=False))\n",
    "    logits_list = expit(logits_list)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Loading dataset for testing\n",
      "====Loading model for testing\n",
      "====Testing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "    calendar_notification      0.267     0.178     0.213        45\n",
      "     transport_directions      0.605     0.561     0.582        41\n",
      "           cooking_recipe      0.711     0.600     0.651        45\n",
      "               radio_play      0.808     0.755     0.781       139\n",
      "             lists_remove      0.928     0.790     0.853        81\n",
      "               news_query      0.673     0.760     0.714       146\n",
      "         cooking_question      0.511     0.511     0.511        45\n",
      "           contacts_query      0.705     0.585     0.639        53\n",
      "             general_joke      0.911     0.911     0.911        45\n",
      "               audio_mute      0.676     0.605     0.639        38\n",
      "            QA_open_query      0.397     0.383     0.390       120\n",
      "          transport_train      0.804     0.863     0.832        95\n",
      "         weather_question      0.648     0.670     0.659        88\n",
      "           music_question      0.659     0.659     0.659        44\n",
      "               QA_factoid      0.754     0.708     0.730       195\n",
      "              email_query      0.923     0.876     0.899       177\n",
      "              lists_query      0.847     0.874     0.860        95\n",
      "     general_conversation      0.445     0.570     0.500       165\n",
      "recommendation_locations       0.756     0.721     0.738        43\n",
      "       calendar_set_event      0.738     0.823     0.778       192\n",
      "          weather_request      0.724     0.702     0.713       168\n",
      "            QA_definition      0.750     0.847     0.795       124\n",
      "           takeaway_query      0.911     0.872     0.891        47\n",
      "                  IOT_hue      0.936     0.958     0.947       214\n",
      "           datetime_query      0.807     0.896     0.849       135\n",
      "              email_reply      0.912     0.721     0.805        43\n",
      "                 QA_maths      0.707     0.763     0.734        38\n",
      "             lists_adding      0.844     0.794     0.818        34\n",
      "             QA_celebrity      0.741     0.741     0.741       108\n",
      "               music_play      0.735     0.820     0.775       244\n",
      "                game_play      0.860     0.875     0.867        56\n",
      "                 IOT_wemo      0.947     0.878     0.911        41\n",
      "          general_mistake      0.500     0.347     0.410        49\n",
      "           music_settings      0.520     0.520     0.520        50\n",
      "              alarm_query      0.600     0.649     0.623        37\n",
      "    recommendation_movies      0.708     0.395     0.507        43\n",
      "    calendar_delete_event      0.837     0.877     0.856       146\n",
      "             social_query      0.773     0.739     0.756        46\n",
      "           takeaway_order      0.800     0.727     0.762        44\n",
      "        transport_traffic      0.844     0.884     0.864        43\n",
      "           audiobook_play      0.896     0.782     0.835        55\n",
      "         email_send_email      0.852     0.897     0.874       116\n",
      "         general_feedback      0.692     0.727     0.709       139\n",
      "               IOT_coffee      0.938     0.900     0.918        50\n",
      "            podcasts_play      0.860     0.771     0.813        96\n",
      "             alarm_remove      0.727     0.600     0.658        40\n",
      "                alarm_set      0.796     0.812     0.804        48\n",
      "           reminder_query      0.711     0.696     0.703        46\n",
      "              social_post      0.861     0.853     0.857       116\n",
      "           lists_creating      0.850     0.810     0.829        42\n",
      "        datetime_question      0.605     0.605     0.605        43\n",
      "                 QA_stock      0.843     0.860     0.851        50\n",
      "        general_confusion      0.564     0.633     0.596        49\n",
      "        calendar_question      0.692     0.643     0.667        42\n",
      "             audio_volume      0.615     0.627     0.621        51\n",
      "    news_set_notification      0.483     0.333     0.394        42\n",
      "   recommendation_events       0.581     0.581     0.581        43\n",
      "        music_preferences      0.671     0.639     0.654        83\n",
      "             IOT_cleaning      0.935     0.896     0.915        48\n",
      "     general_confirmation      0.469     0.349     0.400        43\n",
      "     calendar_query_event      0.564     0.615     0.588       122\n",
      "           transport_taxi      0.943     0.805     0.868        41\n",
      "             reminder_set      0.488     0.568     0.525        74\n",
      "         datetime_convert      0.769     0.571     0.656        35\n",
      "\n",
      "                 accuracy                          0.736      5116\n",
      "                macro avg      0.729     0.703     0.713      5116\n",
      "             weighted avg      0.737     0.736     0.734      5116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(last_saved_model, dataset, batch_size, labels_dict, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
